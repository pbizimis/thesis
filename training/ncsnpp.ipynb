{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NCSNPP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpL9JEM2ME3P",
        "outputId": "e1100414-49f8-4eae-acb3-3e598defad76"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr  8 09:06:54 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.67       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OX8nFlo3NRgM",
        "outputId": "1ddf97f3-7b93-4123-b583-3eadb0a7ee3c"
      },
      "source": [
        "!git clone --recurse-submodules -b develop https://github.com/pbizimis/thesis"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'thesis'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 45 (delta 15), reused 35 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (45/45), done.\n",
            "Submodule 'AdversarialConsistentScoreMatching' (https://github.com/pbizimis/AdversarialConsistentScoreMatching.git) registered for path 'AdversarialConsistentScoreMatching'\n",
            "Submodule 'score_sde_pytorch' (https://github.com/pbizimis/score_sde_pytorch.git) registered for path 'score_sde_pytorch'\n",
            "Submodule 'stylegan2-ada-pytorch' (https://github.com/pbizimis/stylegan2-ada-pytorch.git) registered for path 'stylegan2-ada-pytorch'\n",
            "Cloning into '/content/thesis/AdversarialConsistentScoreMatching'...\n",
            "remote: Enumerating objects: 66, done.        \n",
            "remote: Counting objects: 100% (66/66), done.        \n",
            "remote: Compressing objects: 100% (58/58), done.        \n",
            "remote: Total 150 (delta 29), reused 22 (delta 8), pack-reused 84        \n",
            "Receiving objects: 100% (150/150), 175.15 MiB | 21.36 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n",
            "Cloning into '/content/thesis/score_sde_pytorch'...\n",
            "remote: Enumerating objects: 173, done.        \n",
            "remote: Counting objects: 100% (173/173), done.        \n",
            "remote: Compressing objects: 100% (95/95), done.        \n",
            "remote: Total 173 (delta 95), reused 152 (delta 78), pack-reused 0        \n",
            "Receiving objects: 100% (173/173), 4.02 MiB | 10.54 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n",
            "Cloning into '/content/thesis/stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 187, done.        \n",
            "remote: Total 187 (delta 0), reused 0 (delta 0), pack-reused 187        \n",
            "Receiving objects: 100% (187/187), 1.15 MiB | 12.00 MiB/s, done.\n",
            "Resolving deltas: 100% (93/93), done.\n",
            "Submodule path 'AdversarialConsistentScoreMatching': checked out '1fd85496503602294ddcf32895eae03ef8c190ff'\n",
            "Submodule path 'score_sde_pytorch': checked out 'cb0e019fc7f1262724877730d64f75eb16aab1b0'\n",
            "Submodule path 'stylegan2-ada-pytorch': checked out 'd24f8064e5776acc81b0f50c5ac334f5e8406940'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMoYAs7qNSVs",
        "outputId": "8136ef93-9bb7-41d1-aa79-2c2d9349f8e1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bl-9TvrmNUoF",
        "outputId": "23426a64-3ba4-483e-8bee-1e85e0ca6658"
      },
      "source": [
        "%cd thesis/score_sde_pytorch/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/thesis/score_sde_pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Y51_UmNV-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c493723d-3da8-4c8b-8676-b3c9b0d67871"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ml-collections==0.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/d4/9ab1a8c2aebf78c348404c464733974dc4e7088174d6272ed09c2fa5a8fa/ml_collections-0.1.0-py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 7.1MB/s \n",
            "\u001b[?25hCollecting tensorflow-gan==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/2e/62922111d7d50e1900e3030764743ea7735540ce103b3ab30fd5cd2d8a2b/tensorflow_gan-2.0.0-py2.py3-none-any.whl (365kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 10.3MB/s \n",
            "\u001b[?25hCollecting tensorflow_io\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/41/881ec181816767bd91b8f2dbb319dff8eb5ff80039ed6e003c1ab8d547d7/tensorflow_io-0.17.0-cp37-cp37m-manylinux2010_x86_64.whl (25.3MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3MB 54.9MB/s \n",
            "\u001b[?25hCollecting tensorflow_datasets==3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/99/996b15ff5d11166c3516012838f569f78d57b71d4aac051caea826f6c7e0/tensorflow_datasets-3.1.0-py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 45.7MB/s \n",
            "\u001b[?25hCollecting tensorflow==2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/0a/012cc33c643d844433d13001dd1db179e7020b05ddbbd0a9dc86c38a8efa/tensorflow-2.4.0-cp37-cp37m-manylinux2010_x86_64.whl (394.7MB)\n",
            "\u001b[K     |████████████████████████████████| 394.7MB 37kB/s \n",
            "\u001b[?25hCollecting tensorflow-addons==0.12.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/ff/51d85f54096456bc0b56aa99a2f4da430d5fc514e7e13704a068803d2316/tensorflow_addons-0.12.0-cp37-cp37m-manylinux2010_x86_64.whl (703kB)\n",
            "\u001b[K     |████████████████████████████████| 706kB 58.6MB/s \n",
            "\u001b[?25hCollecting tensorboard==2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/83/179c8f76e5716030cc3ee9433721161cfcc1d854e9ba20c9205180bb100a/tensorboard-2.4.0-py3-none-any.whl (10.6MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6MB 58.1MB/s \n",
            "\u001b[?25hCollecting absl-py==0.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/07/f69dd3367368ad69f174bfe426a973651412ec11d48ec05c000f19fe0561/absl_py-0.10.0-py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 46.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.9.1+cu101)\n",
            "Collecting ninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/de/393468f2a37fc2c1dc3a06afc37775e27fde2d16845424141d4da62c686d/ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 79.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from ml-collections==0.1.0->-r requirements.txt (line 1)) (3.13)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from ml-collections==0.1.0->-r requirements.txt (line 1)) (0.5.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from ml-collections==0.1.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gan==2.0.0->-r requirements.txt (line 2)) (0.11.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gan==2.0.0->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==3.1.0->-r requirements.txt (line 4)) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==3.1.0->-r requirements.txt (line 4)) (3.12.4)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==3.1.0->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==3.1.0->-r requirements.txt (line 4)) (1.19.5)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==3.1.0->-r requirements.txt (line 4)) (20.3.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==3.1.0->-r requirements.txt (line 4)) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==3.1.0->-r requirements.txt (line 4)) (4.41.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==3.1.0->-r requirements.txt (line 4)) (0.3.3)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==3.1.0->-r requirements.txt (line 4)) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==3.1.0->-r requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets==3.1.0->-r requirements.txt (line 4)) (0.29.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->-r requirements.txt (line 5)) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->-r requirements.txt (line 5)) (2.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->-r requirements.txt (line 5)) (1.12)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->-r requirements.txt (line 5)) (1.32.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->-r requirements.txt (line 5)) (3.7.4.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->-r requirements.txt (line 5)) (0.36.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->-r requirements.txt (line 5)) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->-r requirements.txt (line 5)) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->-r requirements.txt (line 5)) (3.3.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->-r requirements.txt (line 5)) (1.6.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons==0.12.0->-r requirements.txt (line 6)) (2.7.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.4.0->-r requirements.txt (line 7)) (0.4.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.4.0->-r requirements.txt (line 7)) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.4.0->-r requirements.txt (line 7)) (54.2.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.4.0->-r requirements.txt (line 7)) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.4.0->-r requirements.txt (line 7)) (1.28.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.4.0->-r requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->-r requirements.txt (line 10)) (7.1.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.7->tensorflow-gan==2.0.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.7->tensorflow-gan==2.0.0->-r requirements.txt (line 2)) (4.4.2)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability>=0.7->tensorflow-gan==2.0.0->-r requirements.txt (line 2)) (0.1.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets==3.1.0->-r requirements.txt (line 4)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets==3.1.0->-r requirements.txt (line 4)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets==3.1.0->-r requirements.txt (line 4)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow_datasets==3.1.0->-r requirements.txt (line 4)) (2020.12.5)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets==3.1.0->-r requirements.txt (line 4)) (1.53.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.4.0->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard==2.4.0->-r requirements.txt (line 7)) (3.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.4.0->-r requirements.txt (line 7)) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.4.0->-r requirements.txt (line 7)) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.4.0->-r requirements.txt (line 7)) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.4.0->-r requirements.txt (line 7)) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard==2.4.0->-r requirements.txt (line 7)) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard==2.4.0->-r requirements.txt (line 7)) (0.4.8)\n",
            "Installing collected packages: absl-py, ml-collections, tensorflow-gan, tensorboard, tensorflow, tensorflow-io, tensorflow-datasets, tensorflow-addons, ninja\n",
            "  Found existing installation: absl-py 0.12.0\n",
            "    Uninstalling absl-py-0.12.0:\n",
            "      Successfully uninstalled absl-py-0.12.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "  Found existing installation: tensorflow-datasets 4.0.1\n",
            "    Uninstalling tensorflow-datasets-4.0.1:\n",
            "      Successfully uninstalled tensorflow-datasets-4.0.1\n",
            "Successfully installed absl-py-0.10.0 ml-collections-0.1.0 ninja-1.10.0.post2 tensorboard-2.4.0 tensorflow-2.4.0 tensorflow-addons-0.12.0 tensorflow-datasets-3.1.0 tensorflow-gan-2.0.0 tensorflow-io-0.17.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWkohFpJio1Z"
      },
      "source": [
        "# !python dataset_tool.py create_from_images /content/drive/MyDrive/Training/tf_dataset /content/drive/MyDrive/Training/set_128x128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s74fFokI_r4q",
        "outputId": "4608e1b3-df6c-443a-e454-59e207d9546b"
      },
      "source": [
        "!python main.py --config=/content/thesis/score_sde_pytorch/configs/ve/custom_ncsnpp_continuous.py --mode=train --workdir=/content/drive/MyDrive/Training/workdir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-08 09:08:41.131953: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_gan/python/estimator/tpu_gan_estimator.py:42: The name tf.estimator.tpu.TPUEstimator is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimator instead.\n",
            "\n",
            "2021-04-08 09:09:41.941835: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-04-08 09:09:41.941954: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-04-08 09:09:41.942060: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 09:09:41.942590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2021-04-08 09:09:41.942624: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-04-08 09:09:42.031334: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-04-08 09:09:42.031449: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-04-08 09:09:42.142222: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-04-08 09:09:42.165859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-04-08 09:09:42.350221: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-04-08 09:09:42.368637: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-04-08 09:09:42.373430: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-04-08 09:09:42.373564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 09:09:42.374168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 09:09:42.374625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-04-08 09:09:42.375001: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-04-08 09:09:42.375242: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-04-08 09:09:42.375350: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 09:09:42.375844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2021-04-08 09:09:42.375875: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-04-08 09:09:42.375911: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-04-08 09:09:42.375934: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-04-08 09:09:42.375950: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-04-08 09:09:42.375966: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-04-08 09:09:42.375981: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-04-08 09:09:42.375996: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-04-08 09:09:42.376012: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-04-08 09:09:42.376063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 09:09:42.376545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 09:09:42.377002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-04-08 09:09:42.377044: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-04-08 09:09:42.877787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-04-08 09:09:42.877839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
            "2021-04-08 09:09:42.877851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
            "2021-04-08 09:09:42.878163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 09:09:42.878778: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 09:09:42.879298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-04-08 09:09:42.879750: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-04-08 09:09:42.879799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 12351 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "2021-04-08 09:09:43.079610: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-04-08 09:09:43.080821: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000165000 Hz\n",
            "I0408 09:09:43.146743 139805357463424 run_lib.py:123] Starting training loop at step 960001.\n",
            "I0408 09:10:27.603903 139805357463424 run_lib.py:133] step: 960050, training_loss: 3.58837e+02\n",
            "I0408 09:11:05.246615 139805357463424 run_lib.py:133] step: 960100, training_loss: 4.52854e+02\n",
            "I0408 09:11:06.273352 139805357463424 run_lib.py:146] step: 960100, eval_loss: 6.14730e+02\n",
            "I0408 09:11:43.947597 139805357463424 run_lib.py:133] step: 960150, training_loss: 5.43150e+02\n",
            "I0408 09:12:21.659929 139805357463424 run_lib.py:133] step: 960200, training_loss: 1.09538e+03\n",
            "I0408 09:12:21.954846 139805357463424 run_lib.py:146] step: 960200, eval_loss: 4.46073e+02\n",
            "I0408 09:12:59.731432 139805357463424 run_lib.py:133] step: 960250, training_loss: 5.02709e+02\n",
            "I0408 09:13:37.436575 139805357463424 run_lib.py:133] step: 960300, training_loss: 3.75046e+02\n",
            "I0408 09:13:37.730981 139805357463424 run_lib.py:146] step: 960300, eval_loss: 1.23287e+03\n",
            "I0408 09:14:15.427176 139805357463424 run_lib.py:133] step: 960350, training_loss: 3.79469e+02\n",
            "I0408 09:14:53.235311 139805357463424 run_lib.py:133] step: 960400, training_loss: 1.14486e+03\n",
            "I0408 09:14:53.531339 139805357463424 run_lib.py:146] step: 960400, eval_loss: 7.57430e+02\n",
            "I0408 09:15:31.263044 139805357463424 run_lib.py:133] step: 960450, training_loss: 7.76793e+02\n",
            "I0408 09:16:08.991165 139805357463424 run_lib.py:133] step: 960500, training_loss: 2.18784e+02\n",
            "I0408 09:16:09.288825 139805357463424 run_lib.py:146] step: 960500, eval_loss: 5.87806e+02\n",
            "I0408 09:16:47.044029 139805357463424 run_lib.py:133] step: 960550, training_loss: 6.73031e+02\n",
            "I0408 09:17:24.881903 139805357463424 run_lib.py:133] step: 960600, training_loss: 5.36581e+02\n",
            "I0408 09:17:25.178911 139805357463424 run_lib.py:146] step: 960600, eval_loss: 1.24828e+03\n",
            "I0408 09:18:02.903674 139805357463424 run_lib.py:133] step: 960650, training_loss: 5.69523e+02\n",
            "I0408 09:18:40.615666 139805357463424 run_lib.py:133] step: 960700, training_loss: 3.10360e+02\n",
            "I0408 09:18:40.911672 139805357463424 run_lib.py:146] step: 960700, eval_loss: 1.23018e+03\n",
            "I0408 09:19:18.658595 139805357463424 run_lib.py:133] step: 960750, training_loss: 5.27638e+02\n",
            "I0408 09:19:56.427303 139805357463424 run_lib.py:133] step: 960800, training_loss: 8.31872e+02\n",
            "I0408 09:19:56.722993 139805357463424 run_lib.py:146] step: 960800, eval_loss: 5.06532e+02\n",
            "I0408 09:20:34.444949 139805357463424 run_lib.py:133] step: 960850, training_loss: 5.75853e+02\n",
            "I0408 09:21:12.157984 139805357463424 run_lib.py:133] step: 960900, training_loss: 7.46988e+02\n",
            "I0408 09:21:12.455254 139805357463424 run_lib.py:146] step: 960900, eval_loss: 4.51865e+02\n",
            "I0408 09:21:50.222563 139805357463424 run_lib.py:133] step: 960950, training_loss: 3.65095e+02\n",
            "I0408 09:22:27.971031 139805357463424 run_lib.py:133] step: 961000, training_loss: 4.30950e+02\n",
            "I0408 09:22:28.268057 139805357463424 run_lib.py:146] step: 961000, eval_loss: 5.91807e+02\n",
            "I0408 09:23:06.010927 139805357463424 run_lib.py:133] step: 961050, training_loss: 8.10462e+02\n",
            "I0408 09:23:43.732876 139805357463424 run_lib.py:133] step: 961100, training_loss: 6.06308e+02\n",
            "I0408 09:23:44.027978 139805357463424 run_lib.py:146] step: 961100, eval_loss: 3.48465e+02\n",
            "I0408 09:24:21.742612 139805357463424 run_lib.py:133] step: 961150, training_loss: 3.13158e+02\n",
            "I0408 09:24:59.450508 139805357463424 run_lib.py:133] step: 961200, training_loss: 1.28596e+02\n",
            "I0408 09:24:59.747530 139805357463424 run_lib.py:146] step: 961200, eval_loss: 1.26081e+03\n",
            "I0408 09:25:37.453305 139805357463424 run_lib.py:133] step: 961250, training_loss: 4.68685e+02\n",
            "I0408 09:26:15.192537 139805357463424 run_lib.py:133] step: 961300, training_loss: 4.35403e+02\n",
            "I0408 09:26:15.488020 139805357463424 run_lib.py:146] step: 961300, eval_loss: 3.63045e+02\n",
            "I0408 09:26:53.270786 139805357463424 run_lib.py:133] step: 961350, training_loss: 6.16243e+02\n",
            "I0408 09:27:30.996428 139805357463424 run_lib.py:133] step: 961400, training_loss: 3.08001e+02\n",
            "I0408 09:27:31.294132 139805357463424 run_lib.py:146] step: 961400, eval_loss: 7.13065e+02\n",
            "I0408 09:28:09.050293 139805357463424 run_lib.py:133] step: 961450, training_loss: 9.20714e+02\n",
            "I0408 09:28:46.785480 139805357463424 run_lib.py:133] step: 961500, training_loss: 4.99413e+02\n",
            "I0408 09:28:47.082033 139805357463424 run_lib.py:146] step: 961500, eval_loss: 1.04777e+03\n",
            "I0408 09:29:24.787764 139805357463424 run_lib.py:133] step: 961550, training_loss: 6.72672e+02\n",
            "I0408 09:30:02.521538 139805357463424 run_lib.py:133] step: 961600, training_loss: 4.16261e+02\n",
            "I0408 09:30:02.817973 139805357463424 run_lib.py:146] step: 961600, eval_loss: 2.25890e+02\n",
            "I0408 09:30:40.578720 139805357463424 run_lib.py:133] step: 961650, training_loss: 1.16604e+03\n",
            "I0408 09:31:18.337384 139805357463424 run_lib.py:133] step: 961700, training_loss: 6.91486e+02\n",
            "I0408 09:31:18.633638 139805357463424 run_lib.py:146] step: 961700, eval_loss: 1.03886e+03\n",
            "I0408 09:31:56.379964 139805357463424 run_lib.py:133] step: 961750, training_loss: 1.41346e+03\n",
            "I0408 09:32:34.107428 139805357463424 run_lib.py:133] step: 961800, training_loss: 1.89583e+02\n",
            "I0408 09:32:34.402996 139805357463424 run_lib.py:146] step: 961800, eval_loss: 2.70659e+02\n",
            "I0408 09:33:12.141745 139805357463424 run_lib.py:133] step: 961850, training_loss: 1.79940e+02\n",
            "I0408 09:33:49.821886 139805357463424 run_lib.py:133] step: 961900, training_loss: 4.61813e+02\n",
            "I0408 09:33:50.117051 139805357463424 run_lib.py:146] step: 961900, eval_loss: 7.16218e+02\n",
            "I0408 09:34:27.866650 139805357463424 run_lib.py:133] step: 961950, training_loss: 7.93990e+02\n",
            "I0408 09:35:05.616085 139805357463424 run_lib.py:133] step: 962000, training_loss: 3.69354e+02\n",
            "I0408 09:35:05.911187 139805357463424 run_lib.py:146] step: 962000, eval_loss: 2.25072e+02\n",
            "I0408 09:35:43.637120 139805357463424 run_lib.py:133] step: 962050, training_loss: 4.14001e+02\n",
            "I0408 09:36:21.363036 139805357463424 run_lib.py:133] step: 962100, training_loss: 7.48882e+02\n",
            "I0408 09:36:21.657261 139805357463424 run_lib.py:146] step: 962100, eval_loss: 6.29704e+02\n",
            "I0408 09:36:59.463728 139805357463424 run_lib.py:133] step: 962150, training_loss: 3.26702e+02\n",
            "I0408 09:37:37.200160 139805357463424 run_lib.py:133] step: 962200, training_loss: 3.66523e+02\n",
            "I0408 09:37:37.495131 139805357463424 run_lib.py:146] step: 962200, eval_loss: 1.03828e+03\n",
            "I0408 09:38:15.230549 139805357463424 run_lib.py:133] step: 962250, training_loss: 8.13033e+02\n",
            "I0408 09:38:52.938107 139805357463424 run_lib.py:133] step: 962300, training_loss: 5.42787e+02\n",
            "I0408 09:38:53.236767 139805357463424 run_lib.py:146] step: 962300, eval_loss: 5.64998e+02\n",
            "I0408 09:39:30.995274 139805357463424 run_lib.py:133] step: 962350, training_loss: 5.91546e+02\n",
            "I0408 09:40:08.762717 139805357463424 run_lib.py:133] step: 962400, training_loss: 5.03947e+02\n",
            "I0408 09:40:09.057414 139805357463424 run_lib.py:146] step: 962400, eval_loss: 1.28736e+03\n",
            "I0408 09:40:46.745520 139805357463424 run_lib.py:133] step: 962450, training_loss: 3.72712e+02\n",
            "I0408 09:41:24.465266 139805357463424 run_lib.py:133] step: 962500, training_loss: 1.43546e+03\n",
            "I0408 09:41:24.759888 139805357463424 run_lib.py:146] step: 962500, eval_loss: 9.42564e+02\n",
            "I0408 09:42:02.462236 139805357463424 run_lib.py:133] step: 962550, training_loss: 8.40089e+02\n",
            "I0408 09:42:40.158000 139805357463424 run_lib.py:133] step: 962600, training_loss: 8.67014e+02\n",
            "I0408 09:42:40.452886 139805357463424 run_lib.py:146] step: 962600, eval_loss: 1.98499e+02\n",
            "I0408 09:43:18.166365 139805357463424 run_lib.py:133] step: 962650, training_loss: 6.48126e+02\n",
            "I0408 09:43:55.838182 139805357463424 run_lib.py:133] step: 962700, training_loss: 6.28815e+02\n",
            "I0408 09:43:56.132570 139805357463424 run_lib.py:146] step: 962700, eval_loss: 7.04639e+02\n",
            "I0408 09:44:33.816433 139805357463424 run_lib.py:133] step: 962750, training_loss: 2.44596e+02\n",
            "I0408 09:45:11.476149 139805357463424 run_lib.py:133] step: 962800, training_loss: 5.25899e+02\n",
            "I0408 09:45:11.770706 139805357463424 run_lib.py:146] step: 962800, eval_loss: 4.69042e+02\n",
            "I0408 09:45:49.434075 139805357463424 run_lib.py:133] step: 962850, training_loss: 7.87824e+02\n",
            "I0408 09:46:27.112917 139805357463424 run_lib.py:133] step: 962900, training_loss: 4.27305e+02\n",
            "I0408 09:46:27.406683 139805357463424 run_lib.py:146] step: 962900, eval_loss: 4.40102e+02\n",
            "I0408 09:47:05.068414 139805357463424 run_lib.py:133] step: 962950, training_loss: 4.82284e+02\n",
            "I0408 09:47:42.753473 139805357463424 run_lib.py:133] step: 963000, training_loss: 1.17036e+03\n",
            "I0408 09:47:43.051722 139805357463424 run_lib.py:146] step: 963000, eval_loss: 7.27023e+02\n",
            "I0408 09:48:20.757648 139805357463424 run_lib.py:133] step: 963050, training_loss: 6.88277e+02\n",
            "I0408 09:48:58.436234 139805357463424 run_lib.py:133] step: 963100, training_loss: 6.79209e+02\n",
            "I0408 09:48:58.730506 139805357463424 run_lib.py:146] step: 963100, eval_loss: 5.79804e+02\n",
            "I0408 09:49:36.383999 139805357463424 run_lib.py:133] step: 963150, training_loss: 3.39157e+02\n",
            "I0408 09:50:14.068743 139805357463424 run_lib.py:133] step: 963200, training_loss: 2.55195e+02\n",
            "I0408 09:50:14.364077 139805357463424 run_lib.py:146] step: 963200, eval_loss: 5.84891e+02\n",
            "I0408 09:50:52.047435 139805357463424 run_lib.py:133] step: 963250, training_loss: 1.08729e+03\n",
            "I0408 09:51:29.746166 139805357463424 run_lib.py:133] step: 963300, training_loss: 4.45651e+02\n",
            "I0408 09:51:30.042329 139805357463424 run_lib.py:146] step: 963300, eval_loss: 5.26586e+02\n",
            "I0408 09:52:07.726539 139805357463424 run_lib.py:133] step: 963350, training_loss: 9.44504e+02\n",
            "I0408 09:52:45.400610 139805357463424 run_lib.py:133] step: 963400, training_loss: 6.18155e+02\n",
            "I0408 09:52:45.694442 139805357463424 run_lib.py:146] step: 963400, eval_loss: 5.79796e+02\n",
            "I0408 09:53:23.402541 139805357463424 run_lib.py:133] step: 963450, training_loss: 6.31232e+02\n",
            "I0408 09:54:01.089545 139805357463424 run_lib.py:133] step: 963500, training_loss: 1.53407e+02\n",
            "I0408 09:54:01.386376 139805357463424 run_lib.py:146] step: 963500, eval_loss: 8.39101e+02\n",
            "I0408 09:54:39.048146 139805357463424 run_lib.py:133] step: 963550, training_loss: 2.45504e+02\n",
            "I0408 09:55:16.721463 139805357463424 run_lib.py:133] step: 963600, training_loss: 6.13772e+02\n",
            "I0408 09:55:17.015189 139805357463424 run_lib.py:146] step: 963600, eval_loss: 1.38729e+03\n",
            "I0408 09:55:54.703983 139805357463424 run_lib.py:133] step: 963650, training_loss: 5.61671e+02\n",
            "I0408 09:56:32.413442 139805357463424 run_lib.py:133] step: 963700, training_loss: 5.05355e+02\n",
            "I0408 09:56:32.709342 139805357463424 run_lib.py:146] step: 963700, eval_loss: 4.79028e+02\n",
            "I0408 09:57:10.419534 139805357463424 run_lib.py:133] step: 963750, training_loss: 2.38584e+02\n",
            "I0408 09:57:48.108707 139805357463424 run_lib.py:133] step: 963800, training_loss: 1.66999e+02\n",
            "I0408 09:57:48.404403 139805357463424 run_lib.py:146] step: 963800, eval_loss: 7.82889e+02\n",
            "I0408 09:58:26.105178 139805357463424 run_lib.py:133] step: 963850, training_loss: 3.46117e+02\n",
            "I0408 09:59:03.814858 139805357463424 run_lib.py:133] step: 963900, training_loss: 6.92016e+02\n",
            "I0408 09:59:04.110453 139805357463424 run_lib.py:146] step: 963900, eval_loss: 4.14018e+02\n",
            "I0408 09:59:41.762826 139805357463424 run_lib.py:133] step: 963950, training_loss: 3.69872e+02\n",
            "I0408 10:00:19.447609 139805357463424 run_lib.py:133] step: 964000, training_loss: 7.22741e+02\n",
            "I0408 10:00:19.742381 139805357463424 run_lib.py:146] step: 964000, eval_loss: 8.07106e+02\n",
            "I0408 10:00:57.439649 139805357463424 run_lib.py:133] step: 964050, training_loss: 2.41931e+02\n",
            "I0408 10:01:35.108993 139805357463424 run_lib.py:133] step: 964100, training_loss: 6.86583e+02\n",
            "I0408 10:01:35.404959 139805357463424 run_lib.py:146] step: 964100, eval_loss: 8.13083e+02\n",
            "I0408 10:02:13.133478 139805357463424 run_lib.py:133] step: 964150, training_loss: 9.02860e+02\n",
            "I0408 10:02:50.789866 139805357463424 run_lib.py:133] step: 964200, training_loss: 4.77272e+02\n",
            "I0408 10:02:51.084879 139805357463424 run_lib.py:146] step: 964200, eval_loss: 6.77719e+02\n",
            "I0408 10:03:28.756401 139805357463424 run_lib.py:133] step: 964250, training_loss: 7.57487e+02\n",
            "I0408 10:04:06.462693 139805357463424 run_lib.py:133] step: 964300, training_loss: 5.61216e+02\n",
            "I0408 10:04:06.758981 139805357463424 run_lib.py:146] step: 964300, eval_loss: 6.50173e+02\n",
            "I0408 10:04:44.428499 139805357463424 run_lib.py:133] step: 964350, training_loss: 4.20482e+02\n",
            "I0408 10:05:22.145691 139805357463424 run_lib.py:133] step: 964400, training_loss: 5.18968e+02\n",
            "I0408 10:05:22.440369 139805357463424 run_lib.py:146] step: 964400, eval_loss: 2.54667e+02\n",
            "I0408 10:06:00.124430 139805357463424 run_lib.py:133] step: 964450, training_loss: 5.52280e+02\n",
            "I0408 10:06:37.849969 139805357463424 run_lib.py:133] step: 964500, training_loss: 4.87918e+02\n",
            "I0408 10:06:38.146438 139805357463424 run_lib.py:146] step: 964500, eval_loss: 9.87834e+02\n",
            "I0408 10:07:15.868478 139805357463424 run_lib.py:133] step: 964550, training_loss: 3.55941e+02\n",
            "I0408 10:07:53.556188 139805357463424 run_lib.py:133] step: 964600, training_loss: 7.01348e+02\n",
            "I0408 10:07:53.854223 139805357463424 run_lib.py:146] step: 964600, eval_loss: 3.63092e+02\n",
            "I0408 10:08:31.553457 139805357463424 run_lib.py:133] step: 964650, training_loss: 4.04942e+02\n",
            "I0408 10:09:09.231547 139805357463424 run_lib.py:133] step: 964700, training_loss: 1.56251e+02\n",
            "I0408 10:09:09.526456 139805357463424 run_lib.py:146] step: 964700, eval_loss: 9.27611e+02\n",
            "I0408 10:09:47.273823 139805357463424 run_lib.py:133] step: 964750, training_loss: 1.20974e+03\n",
            "I0408 10:10:25.007785 139805357463424 run_lib.py:133] step: 964800, training_loss: 4.27300e+02\n",
            "I0408 10:10:25.302546 139805357463424 run_lib.py:146] step: 964800, eval_loss: 6.85287e+02\n",
            "I0408 10:11:03.010438 139805357463424 run_lib.py:133] step: 964850, training_loss: 4.54708e+02\n",
            "I0408 10:11:40.695317 139805357463424 run_lib.py:133] step: 964900, training_loss: 6.72997e+02\n",
            "I0408 10:11:40.989734 139805357463424 run_lib.py:146] step: 964900, eval_loss: 4.09432e+02\n",
            "I0408 10:12:18.732012 139805357463424 run_lib.py:133] step: 964950, training_loss: 6.09345e+02\n",
            "I0408 10:12:56.434221 139805357463424 run_lib.py:133] step: 965000, training_loss: 3.42134e+02\n",
            "I0408 10:13:00.845716 139805357463424 run_lib.py:146] step: 965000, eval_loss: 4.45101e+02\n",
            "I0408 10:13:38.841856 139805357463424 run_lib.py:133] step: 965050, training_loss: 1.68399e+02\n",
            "I0408 10:14:16.549712 139805357463424 run_lib.py:133] step: 965100, training_loss: 8.36943e+02\n",
            "I0408 10:14:16.845427 139805357463424 run_lib.py:146] step: 965100, eval_loss: 6.85362e+02\n",
            "I0408 10:14:54.559565 139805357463424 run_lib.py:133] step: 965150, training_loss: 2.38662e+02\n",
            "I0408 10:15:32.264457 139805357463424 run_lib.py:133] step: 965200, training_loss: 2.94695e+02\n",
            "I0408 10:15:32.558791 139805357463424 run_lib.py:146] step: 965200, eval_loss: 7.90103e+02\n",
            "I0408 10:16:10.260458 139805357463424 run_lib.py:133] step: 965250, training_loss: 4.33384e+02\n",
            "I0408 10:16:47.964930 139805357463424 run_lib.py:133] step: 965300, training_loss: 3.39215e+02\n",
            "I0408 10:16:48.260645 139805357463424 run_lib.py:146] step: 965300, eval_loss: 9.49211e+02\n",
            "I0408 10:17:25.996473 139805357463424 run_lib.py:133] step: 965350, training_loss: 5.95617e+02\n",
            "I0408 10:18:03.732036 139805357463424 run_lib.py:133] step: 965400, training_loss: 3.29546e+02\n",
            "I0408 10:18:04.026851 139805357463424 run_lib.py:146] step: 965400, eval_loss: 4.75696e+02\n",
            "I0408 10:18:41.723226 139805357463424 run_lib.py:133] step: 965450, training_loss: 6.21760e+02\n",
            "I0408 10:19:19.423349 139805357463424 run_lib.py:133] step: 965500, training_loss: 6.37078e+02\n",
            "I0408 10:19:19.718451 139805357463424 run_lib.py:146] step: 965500, eval_loss: 5.74335e+02\n",
            "I0408 10:19:57.422116 139805357463424 run_lib.py:133] step: 965550, training_loss: 5.72734e+02\n",
            "I0408 10:20:35.128256 139805357463424 run_lib.py:133] step: 965600, training_loss: 1.01813e+03\n",
            "I0408 10:20:35.423318 139805357463424 run_lib.py:146] step: 965600, eval_loss: 3.04430e+02\n",
            "I0408 10:21:13.127292 139805357463424 run_lib.py:133] step: 965650, training_loss: 7.08218e+02\n",
            "I0408 10:21:50.815613 139805357463424 run_lib.py:133] step: 965700, training_loss: 6.85176e+02\n",
            "I0408 10:21:51.111106 139805357463424 run_lib.py:146] step: 965700, eval_loss: 9.46835e+02\n",
            "I0408 10:22:28.863240 139805357463424 run_lib.py:133] step: 965750, training_loss: 3.07762e+02\n",
            "I0408 10:23:06.625704 139805357463424 run_lib.py:133] step: 965800, training_loss: 8.61480e+02\n",
            "I0408 10:23:06.922656 139805357463424 run_lib.py:146] step: 965800, eval_loss: 6.80397e+02\n",
            "I0408 10:23:44.636991 139805357463424 run_lib.py:133] step: 965850, training_loss: 6.56328e+02\n",
            "I0408 10:24:22.367197 139805357463424 run_lib.py:133] step: 965900, training_loss: 1.02475e+03\n",
            "I0408 10:24:22.664109 139805357463424 run_lib.py:146] step: 965900, eval_loss: 3.73669e+02\n",
            "I0408 10:25:00.394769 139805357463424 run_lib.py:133] step: 965950, training_loss: 9.59153e+02\n",
            "I0408 10:25:38.070520 139805357463424 run_lib.py:133] step: 966000, training_loss: 5.47526e+02\n",
            "I0408 10:25:38.366048 139805357463424 run_lib.py:146] step: 966000, eval_loss: 7.52398e+02\n",
            "I0408 10:26:16.096903 139805357463424 run_lib.py:133] step: 966050, training_loss: 7.91731e+02\n",
            "I0408 10:26:53.817900 139805357463424 run_lib.py:133] step: 966100, training_loss: 1.04183e+03\n",
            "I0408 10:26:54.113504 139805357463424 run_lib.py:146] step: 966100, eval_loss: 8.42146e+02\n",
            "I0408 10:27:31.867686 139805357463424 run_lib.py:133] step: 966150, training_loss: 6.08074e+02\n",
            "I0408 10:28:09.572173 139805357463424 run_lib.py:133] step: 966200, training_loss: 5.58648e+02\n",
            "I0408 10:28:09.868766 139805357463424 run_lib.py:146] step: 966200, eval_loss: 2.51897e+02\n",
            "I0408 10:28:47.613967 139805357463424 run_lib.py:133] step: 966250, training_loss: 9.00247e+02\n",
            "I0408 10:29:25.321378 139805357463424 run_lib.py:133] step: 966300, training_loss: 8.15444e+02\n",
            "I0408 10:29:25.616566 139805357463424 run_lib.py:146] step: 966300, eval_loss: 4.57056e+02\n",
            "I0408 10:30:03.344816 139805357463424 run_lib.py:133] step: 966350, training_loss: 3.92697e+02\n",
            "I0408 10:30:41.078023 139805357463424 run_lib.py:133] step: 966400, training_loss: 7.99905e+02\n",
            "I0408 10:30:41.376126 139805357463424 run_lib.py:146] step: 966400, eval_loss: 5.55347e+02\n",
            "I0408 10:31:19.145454 139805357463424 run_lib.py:133] step: 966450, training_loss: 5.05137e+02\n",
            "I0408 10:31:56.907271 139805357463424 run_lib.py:133] step: 966500, training_loss: 8.72233e+02\n",
            "I0408 10:31:57.202132 139805357463424 run_lib.py:146] step: 966500, eval_loss: 1.46713e+02\n",
            "I0408 10:32:34.965376 139805357463424 run_lib.py:133] step: 966550, training_loss: 8.27547e+02\n",
            "I0408 10:33:12.717752 139805357463424 run_lib.py:133] step: 966600, training_loss: 5.06153e+02\n",
            "I0408 10:33:13.013843 139805357463424 run_lib.py:146] step: 966600, eval_loss: 8.11987e+02\n",
            "I0408 10:33:50.761753 139805357463424 run_lib.py:133] step: 966650, training_loss: 6.49482e+02\n",
            "I0408 10:34:28.534996 139805357463424 run_lib.py:133] step: 966700, training_loss: 6.11438e+01\n",
            "I0408 10:34:28.831653 139805357463424 run_lib.py:146] step: 966700, eval_loss: 2.96284e+02\n",
            "I0408 10:35:06.560512 139805357463424 run_lib.py:133] step: 966750, training_loss: 4.27548e+02\n",
            "I0408 10:35:44.313820 139805357463424 run_lib.py:133] step: 966800, training_loss: 8.45996e+02\n",
            "I0408 10:35:44.609184 139805357463424 run_lib.py:146] step: 966800, eval_loss: 6.90495e+02\n",
            "I0408 10:36:22.337306 139805357463424 run_lib.py:133] step: 966850, training_loss: 2.38281e+02\n",
            "I0408 10:37:00.118455 139805357463424 run_lib.py:133] step: 966900, training_loss: 2.94453e+02\n",
            "I0408 10:37:00.414802 139805357463424 run_lib.py:146] step: 966900, eval_loss: 7.67006e+02\n",
            "I0408 10:37:38.160555 139805357463424 run_lib.py:133] step: 966950, training_loss: 7.89145e+02\n",
            "I0408 10:38:15.941684 139805357463424 run_lib.py:133] step: 967000, training_loss: 9.77222e+02\n",
            "I0408 10:38:16.239713 139805357463424 run_lib.py:146] step: 967000, eval_loss: 8.93144e+02\n",
            "I0408 10:38:53.988039 139805357463424 run_lib.py:133] step: 967050, training_loss: 7.23883e+02\n",
            "I0408 10:39:31.784724 139805357463424 run_lib.py:133] step: 967100, training_loss: 8.49581e+02\n",
            "I0408 10:39:32.081145 139805357463424 run_lib.py:146] step: 967100, eval_loss: 1.89516e+03\n",
            "I0408 10:40:09.844717 139805357463424 run_lib.py:133] step: 967150, training_loss: 5.57946e+02\n",
            "I0408 10:40:47.631630 139805357463424 run_lib.py:133] step: 967200, training_loss: 7.90252e+02\n",
            "I0408 10:40:47.927694 139805357463424 run_lib.py:146] step: 967200, eval_loss: 8.62836e+02\n",
            "I0408 10:41:25.671329 139805357463424 run_lib.py:133] step: 967250, training_loss: 6.79505e+02\n",
            "I0408 10:42:03.433903 139805357463424 run_lib.py:133] step: 967300, training_loss: 7.99996e+02\n",
            "I0408 10:42:03.730304 139805357463424 run_lib.py:146] step: 967300, eval_loss: 5.50668e+02\n",
            "I0408 10:42:41.633877 139805357463424 run_lib.py:133] step: 967350, training_loss: 1.78389e+02\n",
            "I0408 10:43:19.539381 139805357463424 run_lib.py:133] step: 967400, training_loss: 1.09905e+03\n",
            "I0408 10:43:19.839788 139805357463424 run_lib.py:146] step: 967400, eval_loss: 8.04570e+02\n",
            "I0408 10:43:57.825394 139805357463424 run_lib.py:133] step: 967450, training_loss: 4.56180e+02\n",
            "I0408 10:44:35.673933 139805357463424 run_lib.py:133] step: 967500, training_loss: 1.19716e+03\n",
            "I0408 10:44:35.971120 139805357463424 run_lib.py:146] step: 967500, eval_loss: 3.75440e+02\n",
            "I0408 10:45:13.771143 139805357463424 run_lib.py:133] step: 967550, training_loss: 6.57184e+02\n",
            "I0408 10:45:51.567054 139805357463424 run_lib.py:133] step: 967600, training_loss: 9.51242e+02\n",
            "I0408 10:45:51.863609 139805357463424 run_lib.py:146] step: 967600, eval_loss: 4.05195e+02\n",
            "I0408 10:46:29.642457 139805357463424 run_lib.py:133] step: 967650, training_loss: 3.87065e+02\n",
            "I0408 10:47:07.414135 139805357463424 run_lib.py:133] step: 967700, training_loss: 6.88974e+02\n",
            "I0408 10:47:07.710598 139805357463424 run_lib.py:146] step: 967700, eval_loss: 8.29365e+02\n",
            "I0408 10:47:45.541985 139805357463424 run_lib.py:133] step: 967750, training_loss: 7.96167e+02\n",
            "I0408 10:48:23.539243 139805357463424 run_lib.py:133] step: 967800, training_loss: 4.84245e+02\n",
            "I0408 10:48:23.840484 139805357463424 run_lib.py:146] step: 967800, eval_loss: 2.06598e+02\n",
            "I0408 10:49:01.738050 139805357463424 run_lib.py:133] step: 967850, training_loss: 8.20734e+02\n",
            "I0408 10:49:39.561163 139805357463424 run_lib.py:133] step: 967900, training_loss: 4.17221e+02\n",
            "I0408 10:49:39.857554 139805357463424 run_lib.py:146] step: 967900, eval_loss: 8.72145e+02\n",
            "I0408 10:50:17.721945 139805357463424 run_lib.py:133] step: 967950, training_loss: 2.52423e+02\n",
            "I0408 10:50:55.583429 139805357463424 run_lib.py:133] step: 968000, training_loss: 4.34726e+02\n",
            "I0408 10:50:55.881150 139805357463424 run_lib.py:146] step: 968000, eval_loss: 4.21410e+02\n",
            "I0408 10:51:33.696168 139805357463424 run_lib.py:133] step: 968050, training_loss: 6.82484e+02\n",
            "I0408 10:52:11.580862 139805357463424 run_lib.py:133] step: 968100, training_loss: 7.17211e+02\n",
            "I0408 10:52:11.876874 139805357463424 run_lib.py:146] step: 968100, eval_loss: 9.90255e+02\n",
            "I0408 10:52:49.680002 139805357463424 run_lib.py:133] step: 968150, training_loss: 8.08980e+02\n",
            "I0408 10:53:27.528152 139805357463424 run_lib.py:133] step: 968200, training_loss: 8.85486e+02\n",
            "I0408 10:53:27.824622 139805357463424 run_lib.py:146] step: 968200, eval_loss: 4.01949e+02\n",
            "I0408 10:54:05.603828 139805357463424 run_lib.py:133] step: 968250, training_loss: 4.25168e+02\n",
            "I0408 10:54:43.405753 139805357463424 run_lib.py:133] step: 968300, training_loss: 5.85825e+02\n",
            "I0408 10:54:43.700497 139805357463424 run_lib.py:146] step: 968300, eval_loss: 5.17593e+02\n",
            "I0408 10:55:21.452767 139805357463424 run_lib.py:133] step: 968350, training_loss: 5.38484e+02\n",
            "I0408 10:55:59.260484 139805357463424 run_lib.py:133] step: 968400, training_loss: 7.74242e+02\n",
            "I0408 10:55:59.559804 139805357463424 run_lib.py:146] step: 968400, eval_loss: 6.44243e+02\n",
            "I0408 10:56:37.385804 139805357463424 run_lib.py:133] step: 968450, training_loss: 8.29656e+02\n",
            "I0408 10:57:15.427061 139805357463424 run_lib.py:133] step: 968500, training_loss: 1.29883e+03\n",
            "I0408 10:57:15.725501 139805357463424 run_lib.py:146] step: 968500, eval_loss: 6.01508e+02\n",
            "I0408 10:57:53.728534 139805357463424 run_lib.py:133] step: 968550, training_loss: 5.39831e+02\n",
            "I0408 10:58:31.707323 139805357463424 run_lib.py:133] step: 968600, training_loss: 3.99392e+02\n",
            "I0408 10:58:32.006942 139805357463424 run_lib.py:146] step: 968600, eval_loss: 1.18209e+03\n",
            "I0408 10:59:09.971166 139805357463424 run_lib.py:133] step: 968650, training_loss: 1.77291e+02\n",
            "I0408 10:59:47.815847 139805357463424 run_lib.py:133] step: 968700, training_loss: 3.43599e+02\n",
            "I0408 10:59:48.111841 139805357463424 run_lib.py:146] step: 968700, eval_loss: 5.71416e+02\n",
            "I0408 11:00:25.880647 139805357463424 run_lib.py:133] step: 968750, training_loss: 2.95687e+02\n",
            "I0408 11:01:03.636654 139805357463424 run_lib.py:133] step: 968800, training_loss: 1.25321e+03\n",
            "I0408 11:01:03.932651 139805357463424 run_lib.py:146] step: 968800, eval_loss: 6.69519e+02\n",
            "I0408 11:01:41.786095 139805357463424 run_lib.py:133] step: 968850, training_loss: 7.13166e+02\n",
            "I0408 11:02:19.777013 139805357463424 run_lib.py:133] step: 968900, training_loss: 2.90144e+02\n",
            "I0408 11:02:20.078515 139805357463424 run_lib.py:146] step: 968900, eval_loss: 2.25041e+02\n",
            "I0408 11:02:58.068326 139805357463424 run_lib.py:133] step: 968950, training_loss: 5.81685e+02\n",
            "I0408 11:03:35.914076 139805357463424 run_lib.py:133] step: 969000, training_loss: 8.28752e+02\n",
            "I0408 11:03:36.210966 139805357463424 run_lib.py:146] step: 969000, eval_loss: 8.93637e+02\n",
            "I0408 11:04:14.014544 139805357463424 run_lib.py:133] step: 969050, training_loss: 2.12121e+02\n",
            "I0408 11:04:51.865220 139805357463424 run_lib.py:133] step: 969100, training_loss: 5.70394e+02\n",
            "I0408 11:04:52.160116 139805357463424 run_lib.py:146] step: 969100, eval_loss: 7.03081e+02\n",
            "I0408 11:05:29.940557 139805357463424 run_lib.py:133] step: 969150, training_loss: 5.54358e+02\n",
            "I0408 11:06:07.685030 139805357463424 run_lib.py:133] step: 969200, training_loss: 1.08792e+03\n",
            "I0408 11:06:07.980806 139805357463424 run_lib.py:146] step: 969200, eval_loss: 3.93707e+02\n",
            "I0408 11:06:45.766649 139805357463424 run_lib.py:133] step: 969250, training_loss: 6.82386e+02\n",
            "I0408 11:07:23.567434 139805357463424 run_lib.py:133] step: 969300, training_loss: 4.72333e+02\n",
            "I0408 11:07:23.862947 139805357463424 run_lib.py:146] step: 969300, eval_loss: 9.36238e+02\n",
            "I0408 11:08:01.658911 139805357463424 run_lib.py:133] step: 969350, training_loss: 3.82900e+02\n",
            "I0408 11:08:39.424319 139805357463424 run_lib.py:133] step: 969400, training_loss: 1.19664e+03\n",
            "I0408 11:08:39.721361 139805357463424 run_lib.py:146] step: 969400, eval_loss: 5.51441e+02\n",
            "I0408 11:09:17.513591 139805357463424 run_lib.py:133] step: 969450, training_loss: 7.90408e+02\n",
            "I0408 11:09:55.303492 139805357463424 run_lib.py:133] step: 969500, training_loss: 8.24442e+02\n",
            "I0408 11:09:55.602158 139805357463424 run_lib.py:146] step: 969500, eval_loss: 1.19398e+02\n",
            "I0408 11:10:33.363825 139805357463424 run_lib.py:133] step: 969550, training_loss: 5.89095e+02\n",
            "I0408 11:11:11.162847 139805357463424 run_lib.py:133] step: 969600, training_loss: 4.07583e+02\n",
            "I0408 11:11:11.459476 139805357463424 run_lib.py:146] step: 969600, eval_loss: 5.07473e+02\n",
            "I0408 11:11:49.347711 139805357463424 run_lib.py:133] step: 969650, training_loss: 6.35005e+02\n",
            "I0408 11:12:27.316310 139805357463424 run_lib.py:133] step: 969700, training_loss: 6.50408e+02\n",
            "I0408 11:12:27.615936 139805357463424 run_lib.py:146] step: 969700, eval_loss: 4.64057e+02\n",
            "I0408 11:13:05.616393 139805357463424 run_lib.py:133] step: 969750, training_loss: 8.38849e+02\n",
            "I0408 11:13:43.653796 139805357463424 run_lib.py:133] step: 969800, training_loss: 1.01487e+03\n",
            "I0408 11:13:43.955436 139805357463424 run_lib.py:146] step: 969800, eval_loss: 3.73126e+02\n",
            "I0408 11:14:21.933465 139805357463424 run_lib.py:133] step: 969850, training_loss: 6.84284e+02\n",
            "I0408 11:14:59.894956 139805357463424 run_lib.py:133] step: 969900, training_loss: 5.50339e+02\n",
            "I0408 11:15:00.197125 139805357463424 run_lib.py:146] step: 969900, eval_loss: 4.81328e+02\n",
            "I0408 11:15:38.208282 139805357463424 run_lib.py:133] step: 969950, training_loss: 1.15761e+03\n",
            "I0408 11:16:16.194878 139805357463424 run_lib.py:133] step: 970000, training_loss: 9.27779e+02\n",
            "I0408 11:16:20.549555 139805357463424 run_lib.py:146] step: 970000, eval_loss: 2.45449e+02\n",
            "I0408 11:16:58.917434 139805357463424 run_lib.py:133] step: 970050, training_loss: 3.31931e+02\n",
            "I0408 11:17:36.877120 139805357463424 run_lib.py:133] step: 970100, training_loss: 6.56190e+02\n",
            "I0408 11:17:37.176991 139805357463424 run_lib.py:146] step: 970100, eval_loss: 5.76790e+02\n",
            "I0408 11:18:15.149034 139805357463424 run_lib.py:133] step: 970150, training_loss: 7.78421e+02\n",
            "I0408 11:18:53.000456 139805357463424 run_lib.py:133] step: 970200, training_loss: 4.30500e+02\n",
            "I0408 11:18:53.297793 139805357463424 run_lib.py:146] step: 970200, eval_loss: 6.20513e+02\n",
            "I0408 11:19:31.096770 139805357463424 run_lib.py:133] step: 970250, training_loss: 3.19679e+02\n",
            "I0408 11:20:09.019937 139805357463424 run_lib.py:133] step: 970300, training_loss: 3.22055e+02\n",
            "I0408 11:20:09.317420 139805357463424 run_lib.py:146] step: 970300, eval_loss: 5.53668e+02\n",
            "I0408 11:20:47.325356 139805357463424 run_lib.py:133] step: 970350, training_loss: 4.37693e+02\n",
            "I0408 11:21:25.397762 139805357463424 run_lib.py:133] step: 970400, training_loss: 9.52869e+02\n",
            "I0408 11:21:25.695194 139805357463424 run_lib.py:146] step: 970400, eval_loss: 3.79635e+02\n",
            "I0408 11:22:03.706516 139805357463424 run_lib.py:133] step: 970450, training_loss: 7.13135e+02\n",
            "I0408 11:22:41.691609 139805357463424 run_lib.py:133] step: 970500, training_loss: 4.94647e+02\n",
            "I0408 11:22:41.991581 139805357463424 run_lib.py:146] step: 970500, eval_loss: 3.27700e+02\n",
            "I0408 11:23:19.835268 139805357463424 run_lib.py:133] step: 970550, training_loss: 5.28539e+02\n",
            "I0408 11:23:57.595746 139805357463424 run_lib.py:133] step: 970600, training_loss: 9.36002e+02\n",
            "I0408 11:23:57.894780 139805357463424 run_lib.py:146] step: 970600, eval_loss: 8.08670e+02\n",
            "I0408 11:24:35.611767 139805357463424 run_lib.py:133] step: 970650, training_loss: 7.12940e+02\n",
            "I0408 11:25:13.361572 139805357463424 run_lib.py:133] step: 970700, training_loss: 5.32770e+02\n",
            "I0408 11:25:13.657392 139805357463424 run_lib.py:146] step: 970700, eval_loss: 3.90668e+02\n",
            "I0408 11:25:51.453919 139805357463424 run_lib.py:133] step: 970750, training_loss: 2.93633e+02\n",
            "I0408 11:26:29.212412 139805357463424 run_lib.py:133] step: 970800, training_loss: 5.96563e+02\n",
            "I0408 11:26:29.508507 139805357463424 run_lib.py:146] step: 970800, eval_loss: 3.72324e+02\n",
            "I0408 11:27:07.276791 139805357463424 run_lib.py:133] step: 970850, training_loss: 4.41590e+02\n",
            "I0408 11:27:45.026968 139805357463424 run_lib.py:133] step: 970900, training_loss: 5.75779e+02\n",
            "I0408 11:27:45.327161 139805357463424 run_lib.py:146] step: 970900, eval_loss: 4.40455e+02\n",
            "I0408 11:28:23.125161 139805357463424 run_lib.py:133] step: 970950, training_loss: 1.72813e+02\n",
            "I0408 11:29:01.056662 139805357463424 run_lib.py:133] step: 971000, training_loss: 4.37753e+02\n",
            "I0408 11:29:01.355866 139805357463424 run_lib.py:146] step: 971000, eval_loss: 1.02484e+03\n",
            "I0408 11:29:39.359882 139805357463424 run_lib.py:133] step: 971050, training_loss: 4.99599e+02\n",
            "I0408 11:30:17.323448 139805357463424 run_lib.py:133] step: 971100, training_loss: 3.50582e+02\n",
            "I0408 11:30:17.621417 139805357463424 run_lib.py:146] step: 971100, eval_loss: 6.22739e+02\n",
            "I0408 11:30:55.499915 139805357463424 run_lib.py:133] step: 971150, training_loss: 4.24866e+02\n",
            "I0408 11:31:33.294483 139805357463424 run_lib.py:133] step: 971200, training_loss: 2.85674e+02\n",
            "I0408 11:31:33.590511 139805357463424 run_lib.py:146] step: 971200, eval_loss: 4.00957e+02\n",
            "I0408 11:32:11.348608 139805357463424 run_lib.py:133] step: 971250, training_loss: 3.63162e+02\n",
            "I0408 11:32:49.135278 139805357463424 run_lib.py:133] step: 971300, training_loss: 4.67120e+02\n",
            "I0408 11:32:49.431958 139805357463424 run_lib.py:146] step: 971300, eval_loss: 7.26266e+02\n",
            "I0408 11:33:27.218816 139805357463424 run_lib.py:133] step: 971350, training_loss: 3.83335e+02\n",
            "I0408 11:34:05.019683 139805357463424 run_lib.py:133] step: 971400, training_loss: 4.47054e+02\n",
            "I0408 11:34:05.314981 139805357463424 run_lib.py:146] step: 971400, eval_loss: 8.25051e+02\n",
            "I0408 11:34:43.099682 139805357463424 run_lib.py:133] step: 971450, training_loss: 5.76141e+02\n",
            "I0408 11:35:20.888140 139805357463424 run_lib.py:133] step: 971500, training_loss: 5.13056e+02\n",
            "I0408 11:35:21.184202 139805357463424 run_lib.py:146] step: 971500, eval_loss: 5.03616e+02\n",
            "I0408 11:35:58.921887 139805357463424 run_lib.py:133] step: 971550, training_loss: 1.31400e+02\n",
            "I0408 11:36:36.710817 139805357463424 run_lib.py:133] step: 971600, training_loss: 5.04411e+02\n",
            "I0408 11:36:37.007703 139805357463424 run_lib.py:146] step: 971600, eval_loss: 6.82104e+02\n",
            "I0408 11:37:14.806259 139805357463424 run_lib.py:133] step: 971650, training_loss: 4.20134e+02\n",
            "I0408 11:37:52.645739 139805357463424 run_lib.py:133] step: 971700, training_loss: 6.26228e+02\n",
            "I0408 11:37:52.945158 139805357463424 run_lib.py:146] step: 971700, eval_loss: 5.90105e+02\n",
            "I0408 11:38:30.929912 139805357463424 run_lib.py:133] step: 971750, training_loss: 5.93973e+02\n",
            "I0408 11:39:08.747800 139805357463424 run_lib.py:133] step: 971800, training_loss: 1.10052e+03\n",
            "I0408 11:39:09.044829 139805357463424 run_lib.py:146] step: 971800, eval_loss: 1.03530e+03\n",
            "I0408 11:39:46.776573 139805357463424 run_lib.py:133] step: 971850, training_loss: 5.85963e+02\n",
            "I0408 11:40:24.508565 139805357463424 run_lib.py:133] step: 971900, training_loss: 5.44216e+02\n",
            "I0408 11:40:24.805322 139805357463424 run_lib.py:146] step: 971900, eval_loss: 6.72208e+02\n",
            "I0408 11:41:02.630077 139805357463424 run_lib.py:133] step: 971950, training_loss: 9.68031e+02\n",
            "I0408 11:41:40.382000 139805357463424 run_lib.py:133] step: 972000, training_loss: 6.47014e+02\n",
            "I0408 11:41:40.677368 139805357463424 run_lib.py:146] step: 972000, eval_loss: 8.29213e+02\n",
            "I0408 11:42:18.437282 139805357463424 run_lib.py:133] step: 972050, training_loss: 7.47240e+02\n",
            "I0408 11:42:56.310870 139805357463424 run_lib.py:133] step: 972100, training_loss: 3.49319e+02\n",
            "I0408 11:42:56.610653 139805357463424 run_lib.py:146] step: 972100, eval_loss: 3.08090e+02\n",
            "I0408 11:43:34.505320 139805357463424 run_lib.py:133] step: 972150, training_loss: 4.39538e+02\n",
            "I0408 11:44:12.345426 139805357463424 run_lib.py:133] step: 972200, training_loss: 4.36113e+02\n",
            "I0408 11:44:12.642259 139805357463424 run_lib.py:146] step: 972200, eval_loss: 4.29920e+02\n",
            "I0408 11:44:50.461159 139805357463424 run_lib.py:133] step: 972250, training_loss: 1.11711e+03\n",
            "I0408 11:45:28.399774 139805357463424 run_lib.py:133] step: 972300, training_loss: 4.78352e+02\n",
            "I0408 11:45:28.699172 139805357463424 run_lib.py:146] step: 972300, eval_loss: 4.65986e+02\n",
            "I0408 11:46:06.665200 139805357463424 run_lib.py:133] step: 972350, training_loss: 4.02559e+02\n",
            "I0408 11:46:44.498173 139805357463424 run_lib.py:133] step: 972400, training_loss: 6.00949e+02\n",
            "I0408 11:46:44.793840 139805357463424 run_lib.py:146] step: 972400, eval_loss: 1.00617e+03\n",
            "I0408 11:47:22.566070 139805357463424 run_lib.py:133] step: 972450, training_loss: 6.35157e+02\n",
            "I0408 11:48:00.309215 139805357463424 run_lib.py:133] step: 972500, training_loss: 6.98445e+02\n",
            "I0408 11:48:00.604805 139805357463424 run_lib.py:146] step: 972500, eval_loss: 7.09458e+02\n",
            "I0408 11:48:38.382412 139805357463424 run_lib.py:133] step: 972550, training_loss: 9.18132e+02\n",
            "I0408 11:49:16.183691 139805357463424 run_lib.py:133] step: 972600, training_loss: 5.87298e+02\n",
            "I0408 11:49:16.479761 139805357463424 run_lib.py:146] step: 972600, eval_loss: 4.86928e+02\n",
            "I0408 11:49:54.264842 139805357463424 run_lib.py:133] step: 972650, training_loss: 9.30079e+02\n",
            "I0408 11:50:32.092020 139805357463424 run_lib.py:133] step: 972700, training_loss: 5.45324e+02\n",
            "I0408 11:50:32.386839 139805357463424 run_lib.py:146] step: 972700, eval_loss: 2.26161e+02\n",
            "I0408 11:51:10.160338 139805357463424 run_lib.py:133] step: 972750, training_loss: 5.07174e+02\n",
            "I0408 11:51:48.009143 139805357463424 run_lib.py:133] step: 972800, training_loss: 3.30894e+02\n",
            "I0408 11:51:48.307797 139805357463424 run_lib.py:146] step: 972800, eval_loss: 4.03194e+02\n",
            "I0408 11:52:26.169298 139805357463424 run_lib.py:133] step: 972850, training_loss: 9.64423e+02\n",
            "I0408 11:53:04.173695 139805357463424 run_lib.py:133] step: 972900, training_loss: 3.51604e+02\n",
            "I0408 11:53:04.470294 139805357463424 run_lib.py:146] step: 972900, eval_loss: 3.48522e+02\n",
            "I0408 11:53:42.529757 139805357463424 run_lib.py:133] step: 972950, training_loss: 4.95975e+02\n",
            "I0408 11:54:20.487945 139805357463424 run_lib.py:133] step: 973000, training_loss: 7.41374e+02\n",
            "I0408 11:54:20.784398 139805357463424 run_lib.py:146] step: 973000, eval_loss: 4.43151e+02\n",
            "I0408 11:54:58.622210 139805357463424 run_lib.py:133] step: 973050, training_loss: 8.06243e+02\n",
            "I0408 11:55:36.510445 139805357463424 run_lib.py:133] step: 973100, training_loss: 5.32754e+02\n",
            "I0408 11:55:36.808024 139805357463424 run_lib.py:146] step: 973100, eval_loss: 5.79219e+02\n",
            "I0408 11:56:14.668738 139805357463424 run_lib.py:133] step: 973150, training_loss: 3.89529e+02\n",
            "I0408 11:56:52.541016 139805357463424 run_lib.py:133] step: 973200, training_loss: 9.10836e+02\n",
            "I0408 11:56:52.838922 139805357463424 run_lib.py:146] step: 973200, eval_loss: 6.07591e+02\n",
            "I0408 11:57:30.685837 139805357463424 run_lib.py:133] step: 973250, training_loss: 3.12269e+02\n",
            "I0408 11:58:08.573029 139805357463424 run_lib.py:133] step: 973300, training_loss: 8.55870e+02\n",
            "I0408 11:58:08.870956 139805357463424 run_lib.py:146] step: 973300, eval_loss: 3.90885e+02\n",
            "I0408 11:58:46.753204 139805357463424 run_lib.py:133] step: 973350, training_loss: 4.38840e+02\n",
            "I0408 11:59:24.665647 139805357463424 run_lib.py:133] step: 973400, training_loss: 5.10096e+02\n",
            "I0408 11:59:24.962612 139805357463424 run_lib.py:146] step: 973400, eval_loss: 3.16618e+02\n",
            "I0408 12:00:02.847651 139805357463424 run_lib.py:133] step: 973450, training_loss: 4.17100e+02\n",
            "I0408 12:00:40.724660 139805357463424 run_lib.py:133] step: 973500, training_loss: 6.72509e+02\n",
            "I0408 12:00:41.021976 139805357463424 run_lib.py:146] step: 973500, eval_loss: 5.25831e+02\n",
            "I0408 12:01:18.913539 139805357463424 run_lib.py:133] step: 973550, training_loss: 9.67700e+02\n",
            "I0408 12:01:56.803639 139805357463424 run_lib.py:133] step: 973600, training_loss: 9.71796e+02\n",
            "I0408 12:01:57.100595 139805357463424 run_lib.py:146] step: 973600, eval_loss: 5.03156e+02\n",
            "I0408 12:02:34.955600 139805357463424 run_lib.py:133] step: 973650, training_loss: 7.37602e+02\n",
            "I0408 12:03:12.805963 139805357463424 run_lib.py:133] step: 973700, training_loss: 3.79593e+02\n",
            "I0408 12:03:13.105601 139805357463424 run_lib.py:146] step: 973700, eval_loss: 5.91487e+02\n",
            "I0408 12:03:50.937857 139805357463424 run_lib.py:133] step: 973750, training_loss: 7.07582e+02\n",
            "I0408 12:04:28.801449 139805357463424 run_lib.py:133] step: 973800, training_loss: 2.05008e+02\n",
            "I0408 12:04:29.099173 139805357463424 run_lib.py:146] step: 973800, eval_loss: 6.74524e+02\n",
            "I0408 12:05:06.939870 139805357463424 run_lib.py:133] step: 973850, training_loss: 1.03246e+03\n",
            "I0408 12:05:44.742565 139805357463424 run_lib.py:133] step: 973900, training_loss: 6.37734e+02\n",
            "I0408 12:05:45.041200 139805357463424 run_lib.py:146] step: 973900, eval_loss: 7.69172e+02\n",
            "I0408 12:06:22.854462 139805357463424 run_lib.py:133] step: 973950, training_loss: 6.80470e+02\n",
            "I0408 12:07:00.651370 139805357463424 run_lib.py:133] step: 974000, training_loss: 2.03085e+02\n",
            "I0408 12:07:00.947491 139805357463424 run_lib.py:146] step: 974000, eval_loss: 1.12399e+03\n",
            "I0408 12:07:38.772910 139805357463424 run_lib.py:133] step: 974050, training_loss: 1.01443e+02\n",
            "I0408 12:08:16.570112 139805357463424 run_lib.py:133] step: 974100, training_loss: 8.80349e+02\n",
            "I0408 12:08:16.865935 139805357463424 run_lib.py:146] step: 974100, eval_loss: 3.83285e+02\n",
            "I0408 12:08:54.632036 139805357463424 run_lib.py:133] step: 974150, training_loss: 3.29246e+02\n",
            "I0408 12:09:32.424234 139805357463424 run_lib.py:133] step: 974200, training_loss: 8.65958e+02\n",
            "I0408 12:09:32.720422 139805357463424 run_lib.py:146] step: 974200, eval_loss: 9.24116e+02\n",
            "I0408 12:10:10.568675 139805357463424 run_lib.py:133] step: 974250, training_loss: 9.45055e+02\n",
            "I0408 12:10:48.612722 139805357463424 run_lib.py:133] step: 974300, training_loss: 7.52432e+02\n",
            "I0408 12:10:48.913424 139805357463424 run_lib.py:146] step: 974300, eval_loss: 8.49442e+02\n",
            "I0408 12:11:26.956697 139805357463424 run_lib.py:133] step: 974350, training_loss: 6.84362e+02\n",
            "I0408 12:12:05.008583 139805357463424 run_lib.py:133] step: 974400, training_loss: 5.37083e+02\n",
            "I0408 12:12:05.308028 139805357463424 run_lib.py:146] step: 974400, eval_loss: 8.21335e+02\n",
            "I0408 12:12:43.299369 139805357463424 run_lib.py:133] step: 974450, training_loss: 7.62872e+02\n",
            "I0408 12:13:21.188470 139805357463424 run_lib.py:133] step: 974500, training_loss: 5.26565e+02\n",
            "I0408 12:13:21.484629 139805357463424 run_lib.py:146] step: 974500, eval_loss: 6.55905e+02\n",
            "I0408 12:13:59.249632 139805357463424 run_lib.py:133] step: 974550, training_loss: 5.71497e+02\n",
            "I0408 12:14:37.183404 139805357463424 run_lib.py:133] step: 974600, training_loss: 1.09370e+03\n",
            "I0408 12:14:37.480263 139805357463424 run_lib.py:146] step: 974600, eval_loss: 3.94402e+02\n",
            "I0408 12:15:15.484873 139805357463424 run_lib.py:133] step: 974650, training_loss: 4.64707e+02\n",
            "I0408 12:15:53.477236 139805357463424 run_lib.py:133] step: 974700, training_loss: 1.13224e+03\n",
            "I0408 12:15:53.777022 139805357463424 run_lib.py:146] step: 974700, eval_loss: 1.37136e+03\n",
            "I0408 12:16:31.891085 139805357463424 run_lib.py:133] step: 974750, training_loss: 1.60406e+03\n",
            "I0408 12:17:09.744928 139805357463424 run_lib.py:133] step: 974800, training_loss: 5.02163e+02\n",
            "I0408 12:17:10.041681 139805357463424 run_lib.py:146] step: 974800, eval_loss: 3.37196e+02\n",
            "I0408 12:17:47.930331 139805357463424 run_lib.py:133] step: 974850, training_loss: 4.62785e+02\n",
            "I0408 12:18:25.783807 139805357463424 run_lib.py:133] step: 974900, training_loss: 9.25762e+02\n",
            "I0408 12:18:26.081383 139805357463424 run_lib.py:146] step: 974900, eval_loss: 4.04071e+01\n",
            "I0408 12:19:03.879802 139805357463424 run_lib.py:133] step: 974950, training_loss: 4.63186e+02\n",
            "I0408 12:19:41.700279 139805357463424 run_lib.py:133] step: 975000, training_loss: 7.84867e+02\n",
            "I0408 12:19:46.134162 139805357463424 run_lib.py:146] step: 975000, eval_loss: 2.31335e+02\n",
            "I0408 12:20:24.124485 139805357463424 run_lib.py:133] step: 975050, training_loss: 6.13996e+02\n",
            "I0408 12:21:01.956958 139805357463424 run_lib.py:133] step: 975100, training_loss: 3.62663e+02\n",
            "I0408 12:21:02.254190 139805357463424 run_lib.py:146] step: 975100, eval_loss: 5.17172e+02\n",
            "I0408 12:21:40.063603 139805357463424 run_lib.py:133] step: 975150, training_loss: 1.64965e+02\n",
            "I0408 12:22:17.844184 139805357463424 run_lib.py:133] step: 975200, training_loss: 9.56405e+02\n",
            "I0408 12:22:18.141410 139805357463424 run_lib.py:146] step: 975200, eval_loss: 8.72567e+02\n",
            "I0408 12:22:55.928967 139805357463424 run_lib.py:133] step: 975250, training_loss: 5.71115e+02\n",
            "I0408 12:23:33.713314 139805357463424 run_lib.py:133] step: 975300, training_loss: 9.32633e+02\n",
            "I0408 12:23:34.010392 139805357463424 run_lib.py:146] step: 975300, eval_loss: 4.47689e+02\n",
            "I0408 12:24:11.832981 139805357463424 run_lib.py:133] step: 975350, training_loss: 5.49416e+02\n",
            "I0408 12:24:49.685244 139805357463424 run_lib.py:133] step: 975400, training_loss: 3.17908e+02\n",
            "I0408 12:24:49.983071 139805357463424 run_lib.py:146] step: 975400, eval_loss: 4.26182e+02\n",
            "I0408 12:25:27.745665 139805357463424 run_lib.py:133] step: 975450, training_loss: 1.69331e+03\n",
            "I0408 12:26:05.514620 139805357463424 run_lib.py:133] step: 975500, training_loss: 6.74608e+02\n",
            "I0408 12:26:05.810759 139805357463424 run_lib.py:146] step: 975500, eval_loss: 7.41618e+02\n",
            "I0408 12:26:43.625724 139805357463424 run_lib.py:133] step: 975550, training_loss: 5.87945e+02\n",
            "I0408 12:27:21.400756 139805357463424 run_lib.py:133] step: 975600, training_loss: 8.60741e+02\n",
            "I0408 12:27:21.699111 139805357463424 run_lib.py:146] step: 975600, eval_loss: 2.63938e+02\n",
            "I0408 12:27:59.484111 139805357463424 run_lib.py:133] step: 975650, training_loss: 5.67612e+02\n",
            "I0408 12:28:37.251300 139805357463424 run_lib.py:133] step: 975700, training_loss: 6.20359e+02\n",
            "I0408 12:28:37.548735 139805357463424 run_lib.py:146] step: 975700, eval_loss: 4.91876e+02\n",
            "I0408 12:29:15.325811 139805357463424 run_lib.py:133] step: 975750, training_loss: 2.87714e+02\n",
            "I0408 12:29:53.118833 139805357463424 run_lib.py:133] step: 975800, training_loss: 6.25394e+02\n",
            "I0408 12:29:53.416657 139805357463424 run_lib.py:146] step: 975800, eval_loss: 6.62007e+02\n",
            "I0408 12:30:31.221423 139805357463424 run_lib.py:133] step: 975850, training_loss: 2.24760e+02\n",
            "I0408 12:31:09.016023 139805357463424 run_lib.py:133] step: 975900, training_loss: 4.41527e+02\n",
            "I0408 12:31:09.312618 139805357463424 run_lib.py:146] step: 975900, eval_loss: 9.63499e+02\n",
            "I0408 12:31:47.127503 139805357463424 run_lib.py:133] step: 975950, training_loss: 6.16686e+02\n",
            "I0408 12:32:24.970828 139805357463424 run_lib.py:133] step: 976000, training_loss: 6.26664e+02\n",
            "I0408 12:32:25.267473 139805357463424 run_lib.py:146] step: 976000, eval_loss: 2.99059e+02\n",
            "I0408 12:33:03.188624 139805357463424 run_lib.py:133] step: 976050, training_loss: 7.33174e+02\n",
            "I0408 12:33:41.176630 139805357463424 run_lib.py:133] step: 976100, training_loss: 5.94159e+02\n",
            "I0408 12:33:41.472859 139805357463424 run_lib.py:146] step: 976100, eval_loss: 6.30868e+02\n",
            "I0408 12:34:19.467940 139805357463424 run_lib.py:133] step: 976150, training_loss: 6.56709e+02\n",
            "I0408 12:34:57.445824 139805357463424 run_lib.py:133] step: 976200, training_loss: 6.00815e+02\n",
            "I0408 12:34:57.747055 139805357463424 run_lib.py:146] step: 976200, eval_loss: 1.22110e+03\n",
            "I0408 12:35:35.630734 139805357463424 run_lib.py:133] step: 976250, training_loss: 3.45075e+02\n",
            "I0408 12:36:13.426562 139805357463424 run_lib.py:133] step: 976300, training_loss: 1.48546e+02\n",
            "I0408 12:36:13.723730 139805357463424 run_lib.py:146] step: 976300, eval_loss: 5.40158e+02\n",
            "I0408 12:36:51.527272 139805357463424 run_lib.py:133] step: 976350, training_loss: 4.74139e+02\n",
            "I0408 12:37:29.331834 139805357463424 run_lib.py:133] step: 976400, training_loss: 7.33778e+02\n",
            "I0408 12:37:29.628440 139805357463424 run_lib.py:146] step: 976400, eval_loss: 3.20003e+02\n",
            "I0408 12:38:07.544745 139805357463424 run_lib.py:133] step: 976450, training_loss: 5.30485e+02\n",
            "I0408 12:38:45.507951 139805357463424 run_lib.py:133] step: 976500, training_loss: 7.98961e+02\n",
            "I0408 12:38:45.808117 139805357463424 run_lib.py:146] step: 976500, eval_loss: 5.98291e+02\n",
            "I0408 12:39:23.774551 139805357463424 run_lib.py:133] step: 976550, training_loss: 3.45283e+02\n",
            "I0408 12:40:01.730586 139805357463424 run_lib.py:133] step: 976600, training_loss: 7.28548e+02\n",
            "I0408 12:40:02.026504 139805357463424 run_lib.py:146] step: 976600, eval_loss: 1.85266e+02\n",
            "I0408 12:40:39.790224 139805357463424 run_lib.py:133] step: 976650, training_loss: 6.81703e+02\n",
            "I0408 12:41:17.648965 139805357463424 run_lib.py:133] step: 976700, training_loss: 7.83221e+02\n",
            "I0408 12:41:17.945412 139805357463424 run_lib.py:146] step: 976700, eval_loss: 6.38550e+02\n",
            "I0408 12:41:55.717142 139805357463424 run_lib.py:133] step: 976750, training_loss: 5.90755e+02\n",
            "I0408 12:42:33.510601 139805357463424 run_lib.py:133] step: 976800, training_loss: 8.85098e+02\n",
            "I0408 12:42:33.807635 139805357463424 run_lib.py:146] step: 976800, eval_loss: 1.05894e+03\n",
            "I0408 12:43:11.706339 139805357463424 run_lib.py:133] step: 976850, training_loss: 4.37368e+02\n",
            "I0408 12:43:49.647701 139805357463424 run_lib.py:133] step: 976900, training_loss: 6.05864e+02\n",
            "I0408 12:43:49.945187 139805357463424 run_lib.py:146] step: 976900, eval_loss: 1.08508e+02\n",
            "I0408 12:44:27.931979 139805357463424 run_lib.py:133] step: 976950, training_loss: 5.90113e+02\n",
            "I0408 12:45:05.828602 139805357463424 run_lib.py:133] step: 977000, training_loss: 2.17458e+02\n",
            "I0408 12:45:06.127586 139805357463424 run_lib.py:146] step: 977000, eval_loss: 9.96365e+02\n",
            "I0408 12:45:43.890309 139805357463424 run_lib.py:133] step: 977050, training_loss: 3.75755e+02\n",
            "I0408 12:46:21.635758 139805357463424 run_lib.py:133] step: 977100, training_loss: 6.00586e+02\n",
            "I0408 12:46:21.931647 139805357463424 run_lib.py:146] step: 977100, eval_loss: 1.08894e+03\n",
            "I0408 12:46:59.718941 139805357463424 run_lib.py:133] step: 977150, training_loss: 9.71352e+02\n",
            "I0408 12:47:37.670563 139805357463424 run_lib.py:133] step: 977200, training_loss: 4.82905e+02\n",
            "I0408 12:47:37.966916 139805357463424 run_lib.py:146] step: 977200, eval_loss: 7.10233e+02\n",
            "I0408 12:48:15.734125 139805357463424 run_lib.py:133] step: 977250, training_loss: 5.71125e+02\n",
            "I0408 12:48:53.476673 139805357463424 run_lib.py:133] step: 977300, training_loss: 1.01247e+03\n",
            "I0408 12:48:53.774643 139805357463424 run_lib.py:146] step: 977300, eval_loss: 8.75422e+02\n",
            "I0408 12:49:31.559378 139805357463424 run_lib.py:133] step: 977350, training_loss: 1.36298e+03\n",
            "I0408 12:50:09.323682 139805357463424 run_lib.py:133] step: 977400, training_loss: 5.98237e+02\n",
            "I0408 12:50:09.621176 139805357463424 run_lib.py:146] step: 977400, eval_loss: 4.33488e+02\n",
            "I0408 12:50:47.372957 139805357463424 run_lib.py:133] step: 977450, training_loss: 3.74787e+02\n",
            "I0408 12:51:25.142989 139805357463424 run_lib.py:133] step: 977500, training_loss: 1.20711e+03\n",
            "I0408 12:51:25.440507 139805357463424 run_lib.py:146] step: 977500, eval_loss: 2.86093e+02\n",
            "I0408 12:52:03.215464 139805357463424 run_lib.py:133] step: 977550, training_loss: 4.91971e+02\n",
            "I0408 12:52:41.037900 139805357463424 run_lib.py:133] step: 977600, training_loss: 4.98635e+02\n",
            "I0408 12:52:41.339556 139805357463424 run_lib.py:146] step: 977600, eval_loss: 5.94651e+02\n",
            "I0408 12:53:19.087306 139805357463424 run_lib.py:133] step: 977650, training_loss: 9.08401e+02\n",
            "I0408 12:53:56.865292 139805357463424 run_lib.py:133] step: 977700, training_loss: 2.51909e+02\n",
            "I0408 12:53:57.162107 139805357463424 run_lib.py:146] step: 977700, eval_loss: 9.08221e+02\n",
            "I0408 12:54:34.936161 139805357463424 run_lib.py:133] step: 977750, training_loss: 1.19213e+03\n",
            "I0408 12:55:12.750045 139805357463424 run_lib.py:133] step: 977800, training_loss: 1.00433e+03\n",
            "I0408 12:55:13.047259 139805357463424 run_lib.py:146] step: 977800, eval_loss: 4.31372e+02\n",
            "I0408 12:55:50.822600 139805357463424 run_lib.py:133] step: 977850, training_loss: 9.34849e+02\n",
            "I0408 12:56:28.622102 139805357463424 run_lib.py:133] step: 977900, training_loss: 3.37846e+02\n",
            "I0408 12:56:28.917778 139805357463424 run_lib.py:146] step: 977900, eval_loss: 5.51965e+02\n",
            "I0408 12:57:06.747664 139805357463424 run_lib.py:133] step: 977950, training_loss: 8.00877e+02\n",
            "I0408 12:57:44.581152 139805357463424 run_lib.py:133] step: 978000, training_loss: 8.76416e+02\n",
            "I0408 12:57:44.881258 139805357463424 run_lib.py:146] step: 978000, eval_loss: 1.01858e+03\n",
            "I0408 12:58:22.654585 139805357463424 run_lib.py:133] step: 978050, training_loss: 2.11778e+02\n",
            "I0408 12:59:00.446482 139805357463424 run_lib.py:133] step: 978100, training_loss: 5.93298e+02\n",
            "I0408 12:59:00.743305 139805357463424 run_lib.py:146] step: 978100, eval_loss: 7.11207e+02\n",
            "I0408 12:59:38.563550 139805357463424 run_lib.py:133] step: 978150, training_loss: 8.96578e+02\n",
            "I0408 13:00:16.326400 139805357463424 run_lib.py:133] step: 978200, training_loss: 2.88311e+02\n",
            "I0408 13:00:16.622897 139805357463424 run_lib.py:146] step: 978200, eval_loss: 8.53370e+02\n",
            "I0408 13:00:54.396687 139805357463424 run_lib.py:133] step: 978250, training_loss: 4.80793e+02\n",
            "I0408 13:01:32.202202 139805357463424 run_lib.py:133] step: 978300, training_loss: 6.77996e+02\n",
            "I0408 13:01:32.498801 139805357463424 run_lib.py:146] step: 978300, eval_loss: 4.85535e+02\n",
            "I0408 13:02:10.356731 139805357463424 run_lib.py:133] step: 978350, training_loss: 8.85513e+02\n",
            "I0408 13:02:48.198198 139805357463424 run_lib.py:133] step: 978400, training_loss: 9.00847e+02\n",
            "I0408 13:02:48.494426 139805357463424 run_lib.py:146] step: 978400, eval_loss: 4.34505e+02\n",
            "I0408 13:03:26.285668 139805357463424 run_lib.py:133] step: 978450, training_loss: 1.53561e+03\n",
            "I0408 13:04:04.039444 139805357463424 run_lib.py:133] step: 978500, training_loss: 6.01423e+02\n",
            "I0408 13:04:04.336840 139805357463424 run_lib.py:146] step: 978500, eval_loss: 4.21901e+02\n",
            "I0408 13:04:42.116817 139805357463424 run_lib.py:133] step: 978550, training_loss: 4.36624e+02\n",
            "I0408 13:05:19.879347 139805357463424 run_lib.py:133] step: 978600, training_loss: 4.77808e+02\n",
            "I0408 13:05:20.177351 139805357463424 run_lib.py:146] step: 978600, eval_loss: 6.06409e+02\n",
            "I0408 13:05:57.965984 139805357463424 run_lib.py:133] step: 978650, training_loss: 6.58720e+02\n",
            "I0408 13:06:35.772504 139805357463424 run_lib.py:133] step: 978700, training_loss: 8.13776e+02\n",
            "I0408 13:06:36.069955 139805357463424 run_lib.py:146] step: 978700, eval_loss: 6.71474e+02\n",
            "I0408 13:07:13.851805 139805357463424 run_lib.py:133] step: 978750, training_loss: 7.96827e+02\n",
            "I0408 13:07:51.629835 139805357463424 run_lib.py:133] step: 978800, training_loss: 2.36401e+02\n",
            "I0408 13:07:51.926669 139805357463424 run_lib.py:146] step: 978800, eval_loss: 5.52032e+02\n",
            "I0408 13:08:29.760839 139805357463424 run_lib.py:133] step: 978850, training_loss: 6.49790e+02\n",
            "I0408 13:09:07.538057 139805357463424 run_lib.py:133] step: 978900, training_loss: 1.70387e+03\n",
            "I0408 13:09:07.839027 139805357463424 run_lib.py:146] step: 978900, eval_loss: 6.51292e+02\n",
            "I0408 13:09:45.619971 139805357463424 run_lib.py:133] step: 978950, training_loss: 3.46659e+02\n",
            "I0408 13:10:23.429939 139805357463424 run_lib.py:133] step: 979000, training_loss: 1.22127e+03\n",
            "I0408 13:10:23.726302 139805357463424 run_lib.py:146] step: 979000, eval_loss: 1.52456e+02\n",
            "I0408 13:11:01.477611 139805357463424 run_lib.py:133] step: 979050, training_loss: 3.09133e+02\n",
            "I0408 13:11:39.269991 139805357463424 run_lib.py:133] step: 979100, training_loss: 4.61716e+02\n",
            "I0408 13:11:39.566082 139805357463424 run_lib.py:146] step: 979100, eval_loss: 3.39998e+02\n",
            "I0408 13:12:17.350786 139805357463424 run_lib.py:133] step: 979150, training_loss: 3.29315e+02\n",
            "I0408 13:12:55.115897 139805357463424 run_lib.py:133] step: 979200, training_loss: 9.06511e+02\n",
            "I0408 13:12:55.412360 139805357463424 run_lib.py:146] step: 979200, eval_loss: 3.41988e+02\n",
            "I0408 13:13:33.201355 139805357463424 run_lib.py:133] step: 979250, training_loss: 3.00874e+02\n",
            "I0408 13:14:10.965879 139805357463424 run_lib.py:133] step: 979300, training_loss: 5.95314e+02\n",
            "I0408 13:14:11.261899 139805357463424 run_lib.py:146] step: 979300, eval_loss: 6.90159e+02\n",
            "I0408 13:14:49.031466 139805357463424 run_lib.py:133] step: 979350, training_loss: 3.52232e+02\n",
            "I0408 13:15:26.826448 139805357463424 run_lib.py:133] step: 979400, training_loss: 6.94432e+02\n",
            "I0408 13:15:27.123344 139805357463424 run_lib.py:146] step: 979400, eval_loss: 1.12394e+03\n",
            "I0408 13:16:04.884418 139805357463424 run_lib.py:133] step: 979450, training_loss: 6.72721e+02\n",
            "I0408 13:16:42.648843 139805357463424 run_lib.py:133] step: 979500, training_loss: 6.51138e+02\n",
            "I0408 13:16:42.947145 139805357463424 run_lib.py:146] step: 979500, eval_loss: 4.06681e+02\n",
            "I0408 13:17:20.779237 139805357463424 run_lib.py:133] step: 979550, training_loss: 7.96779e+02\n",
            "I0408 13:17:58.614967 139805357463424 run_lib.py:133] step: 979600, training_loss: 5.60745e+02\n",
            "I0408 13:17:58.912794 139805357463424 run_lib.py:146] step: 979600, eval_loss: 2.89467e+02\n",
            "I0408 13:18:36.721677 139805357463424 run_lib.py:133] step: 979650, training_loss: 5.23373e+02\n",
            "I0408 13:19:14.464639 139805357463424 run_lib.py:133] step: 979700, training_loss: 4.21324e+02\n",
            "I0408 13:19:14.760116 139805357463424 run_lib.py:146] step: 979700, eval_loss: 7.82885e+02\n",
            "I0408 13:19:52.579637 139805357463424 run_lib.py:133] step: 979750, training_loss: 7.65890e+02\n",
            "I0408 13:20:30.381657 139805357463424 run_lib.py:133] step: 979800, training_loss: 7.96131e+02\n",
            "I0408 13:20:30.678195 139805357463424 run_lib.py:146] step: 979800, eval_loss: 2.67218e+02\n",
            "I0408 13:21:08.437689 139805357463424 run_lib.py:133] step: 979850, training_loss: 3.22989e+02\n",
            "I0408 13:21:46.209978 139805357463424 run_lib.py:133] step: 979900, training_loss: 6.33178e+02\n",
            "I0408 13:21:46.507124 139805357463424 run_lib.py:146] step: 979900, eval_loss: 1.28356e+03\n",
            "I0408 13:22:24.310034 139805357463424 run_lib.py:133] step: 979950, training_loss: 5.98741e+02\n",
            "I0408 13:23:02.082314 139805357463424 run_lib.py:133] step: 980000, training_loss: 5.17946e+02\n",
            "I0408 13:23:06.211061 139805357463424 run_lib.py:146] step: 980000, eval_loss: 5.92648e+02\n",
            "I0408 13:23:44.300018 139805357463424 run_lib.py:133] step: 980050, training_loss: 6.01492e+02\n",
            "I0408 13:24:22.078341 139805357463424 run_lib.py:133] step: 980100, training_loss: 1.14879e+03\n",
            "I0408 13:24:22.375029 139805357463424 run_lib.py:146] step: 980100, eval_loss: 9.05036e+02\n",
            "I0408 13:25:00.175317 139805357463424 run_lib.py:133] step: 980150, training_loss: 9.61075e+02\n",
            "I0408 13:25:37.933034 139805357463424 run_lib.py:133] step: 980200, training_loss: 1.12030e+03\n",
            "I0408 13:25:38.230138 139805357463424 run_lib.py:146] step: 980200, eval_loss: 5.20292e+02\n",
            "I0408 13:26:16.036282 139805357463424 run_lib.py:133] step: 980250, training_loss: 3.95840e+02\n",
            "I0408 13:26:53.813389 139805357463424 run_lib.py:133] step: 980300, training_loss: 2.21800e+02\n",
            "I0408 13:26:54.115957 139805357463424 run_lib.py:146] step: 980300, eval_loss: 5.59062e+02\n",
            "I0408 13:27:31.895813 139805357463424 run_lib.py:133] step: 980350, training_loss: 5.77750e+02\n",
            "I0408 13:28:09.643600 139805357463424 run_lib.py:133] step: 980400, training_loss: 3.37941e+02\n",
            "I0408 13:28:09.940577 139805357463424 run_lib.py:146] step: 980400, eval_loss: 3.63186e+02\n",
            "I0408 13:28:47.708034 139805357463424 run_lib.py:133] step: 980450, training_loss: 5.82553e+02\n",
            "I0408 13:29:25.472392 139805357463424 run_lib.py:133] step: 980500, training_loss: 6.94238e+02\n",
            "I0408 13:29:25.767822 139805357463424 run_lib.py:146] step: 980500, eval_loss: 5.45672e+02\n",
            "I0408 13:30:03.539211 139805357463424 run_lib.py:133] step: 980550, training_loss: 5.33732e+02\n",
            "I0408 13:30:41.317947 139805357463424 run_lib.py:133] step: 980600, training_loss: 3.27405e+02\n",
            "I0408 13:30:41.614784 139805357463424 run_lib.py:146] step: 980600, eval_loss: 7.38051e+02\n",
            "I0408 13:31:19.411738 139805357463424 run_lib.py:133] step: 980650, training_loss: 3.31302e+02\n",
            "I0408 13:31:57.216669 139805357463424 run_lib.py:133] step: 980700, training_loss: 4.74243e+02\n",
            "I0408 13:31:57.513219 139805357463424 run_lib.py:146] step: 980700, eval_loss: 6.32260e+02\n",
            "I0408 13:32:35.299937 139805357463424 run_lib.py:133] step: 980750, training_loss: 1.30217e+02\n",
            "I0408 13:33:13.089715 139805357463424 run_lib.py:133] step: 980800, training_loss: 2.85561e+02\n",
            "I0408 13:33:13.385931 139805357463424 run_lib.py:146] step: 980800, eval_loss: 6.99281e+02\n",
            "I0408 13:33:51.132538 139805357463424 run_lib.py:133] step: 980850, training_loss: 5.50418e+02\n",
            "I0408 13:34:28.928218 139805357463424 run_lib.py:133] step: 980900, training_loss: 5.55619e+02\n",
            "I0408 13:34:29.225270 139805357463424 run_lib.py:146] step: 980900, eval_loss: 2.30272e+02\n",
            "I0408 13:35:06.989723 139805357463424 run_lib.py:133] step: 980950, training_loss: 8.38200e+02\n",
            "I0408 13:35:44.797712 139805357463424 run_lib.py:133] step: 981000, training_loss: 6.79669e+02\n",
            "I0408 13:35:45.095186 139805357463424 run_lib.py:146] step: 981000, eval_loss: 6.64847e+02\n",
            "I0408 13:36:22.852219 139805357463424 run_lib.py:133] step: 981050, training_loss: 1.40671e+03\n",
            "I0408 13:37:00.625537 139805357463424 run_lib.py:133] step: 981100, training_loss: 5.35665e+02\n",
            "I0408 13:37:00.923756 139805357463424 run_lib.py:146] step: 981100, eval_loss: 5.26345e+02\n",
            "I0408 13:37:38.743020 139805357463424 run_lib.py:133] step: 981150, training_loss: 3.44053e+02\n",
            "I0408 13:38:16.526252 139805357463424 run_lib.py:133] step: 981200, training_loss: 6.15333e+02\n",
            "I0408 13:38:16.823278 139805357463424 run_lib.py:146] step: 981200, eval_loss: 3.78575e+02\n",
            "I0408 13:38:54.562083 139805357463424 run_lib.py:133] step: 981250, training_loss: 7.94642e+01\n",
            "I0408 13:39:32.363024 139805357463424 run_lib.py:133] step: 981300, training_loss: 3.54628e+02\n",
            "I0408 13:39:32.662032 139805357463424 run_lib.py:146] step: 981300, eval_loss: 6.64728e+02\n",
            "I0408 13:40:10.461504 139805357463424 run_lib.py:133] step: 981350, training_loss: 6.26261e+02\n",
            "I0408 13:40:48.222238 139805357463424 run_lib.py:133] step: 981400, training_loss: 1.09404e+03\n",
            "I0408 13:40:48.519366 139805357463424 run_lib.py:146] step: 981400, eval_loss: 3.18298e+02\n",
            "I0408 13:41:26.308210 139805357463424 run_lib.py:133] step: 981450, training_loss: 1.39311e+03\n",
            "I0408 13:42:04.053055 139805357463424 run_lib.py:133] step: 981500, training_loss: 4.79993e+02\n",
            "I0408 13:42:04.351116 139805357463424 run_lib.py:146] step: 981500, eval_loss: 2.58082e+02\n",
            "I0408 13:42:42.147518 139805357463424 run_lib.py:133] step: 981550, training_loss: 3.98462e+02\n",
            "I0408 13:43:19.917793 139805357463424 run_lib.py:133] step: 981600, training_loss: 8.04543e+02\n",
            "I0408 13:43:20.215476 139805357463424 run_lib.py:146] step: 981600, eval_loss: 8.23410e+02\n",
            "I0408 13:43:57.961718 139805357463424 run_lib.py:133] step: 981650, training_loss: 7.47174e+02\n",
            "I0408 13:44:35.735760 139805357463424 run_lib.py:133] step: 981700, training_loss: 7.93586e+02\n",
            "I0408 13:44:36.030735 139805357463424 run_lib.py:146] step: 981700, eval_loss: 5.24521e+02\n",
            "I0408 13:45:13.792886 139805357463424 run_lib.py:133] step: 981750, training_loss: 6.27746e+02\n",
            "I0408 13:45:51.519939 139805357463424 run_lib.py:133] step: 981800, training_loss: 6.72796e+02\n",
            "I0408 13:45:51.816385 139805357463424 run_lib.py:146] step: 981800, eval_loss: 4.72474e+02\n",
            "I0408 13:46:29.580826 139805357463424 run_lib.py:133] step: 981850, training_loss: 7.65751e+02\n",
            "I0408 13:47:07.312970 139805357463424 run_lib.py:133] step: 981900, training_loss: 2.94843e+02\n",
            "I0408 13:47:07.609074 139805357463424 run_lib.py:146] step: 981900, eval_loss: 4.98329e+02\n",
            "I0408 13:47:45.394397 139805357463424 run_lib.py:133] step: 981950, training_loss: 4.78327e+02\n",
            "I0408 13:48:23.157471 139805357463424 run_lib.py:133] step: 982000, training_loss: 7.47426e+02\n",
            "I0408 13:48:23.453568 139805357463424 run_lib.py:146] step: 982000, eval_loss: 1.21791e+03\n",
            "I0408 13:49:01.186469 139805357463424 run_lib.py:133] step: 982050, training_loss: 1.27379e+03\n",
            "I0408 13:49:38.963813 139805357463424 run_lib.py:133] step: 982100, training_loss: 5.29184e+02\n",
            "I0408 13:49:39.260380 139805357463424 run_lib.py:146] step: 982100, eval_loss: 9.04313e+02\n",
            "I0408 13:50:17.017354 139805357463424 run_lib.py:133] step: 982150, training_loss: 1.14719e+02\n",
            "I0408 13:50:54.750611 139805357463424 run_lib.py:133] step: 982200, training_loss: 1.41170e+03\n",
            "I0408 13:50:55.048243 139805357463424 run_lib.py:146] step: 982200, eval_loss: 1.12497e+02\n",
            "I0408 13:51:32.800368 139805357463424 run_lib.py:133] step: 982250, training_loss: 6.30690e+02\n",
            "I0408 13:52:10.571132 139805357463424 run_lib.py:133] step: 982300, training_loss: 2.56668e+02\n",
            "I0408 13:52:10.874365 139805357463424 run_lib.py:146] step: 982300, eval_loss: 3.19301e+02\n",
            "I0408 13:52:48.622277 139805357463424 run_lib.py:133] step: 982350, training_loss: 1.23335e+03\n",
            "I0408 13:53:26.381628 139805357463424 run_lib.py:133] step: 982400, training_loss: 9.18479e+02\n",
            "I0408 13:53:26.678259 139805357463424 run_lib.py:146] step: 982400, eval_loss: 3.60987e+02\n",
            "I0408 13:54:04.425922 139805357463424 run_lib.py:133] step: 982450, training_loss: 8.72939e+02\n",
            "I0408 13:54:42.185039 139805357463424 run_lib.py:133] step: 982500, training_loss: 8.78949e+02\n",
            "I0408 13:54:42.480433 139805357463424 run_lib.py:146] step: 982500, eval_loss: 7.46303e+02\n",
            "I0408 13:55:20.214740 139805357463424 run_lib.py:133] step: 982550, training_loss: 9.18769e+02\n",
            "I0408 13:55:57.988857 139805357463424 run_lib.py:133] step: 982600, training_loss: 6.26575e+02\n",
            "I0408 13:55:58.285758 139805357463424 run_lib.py:146] step: 982600, eval_loss: 5.90319e+02\n",
            "I0408 13:56:36.027933 139805357463424 run_lib.py:133] step: 982650, training_loss: 6.16732e+02\n",
            "I0408 13:57:13.766258 139805357463424 run_lib.py:133] step: 982700, training_loss: 1.95517e+02\n",
            "I0408 13:57:14.063999 139805357463424 run_lib.py:146] step: 982700, eval_loss: 4.32425e+02\n",
            "I0408 13:57:51.826070 139805357463424 run_lib.py:133] step: 982750, training_loss: 4.07365e+02\n",
            "I0408 13:58:29.558622 139805357463424 run_lib.py:133] step: 982800, training_loss: 5.05967e+02\n",
            "I0408 13:58:29.854819 139805357463424 run_lib.py:146] step: 982800, eval_loss: 3.28859e+02\n",
            "I0408 13:59:07.595506 139805357463424 run_lib.py:133] step: 982850, training_loss: 2.08290e+02\n",
            "I0408 13:59:45.325902 139805357463424 run_lib.py:133] step: 982900, training_loss: 3.26256e+02\n",
            "I0408 13:59:45.621714 139805357463424 run_lib.py:146] step: 982900, eval_loss: 8.18892e+02\n",
            "I0408 14:00:23.377944 139805357463424 run_lib.py:133] step: 982950, training_loss: 7.84034e+02\n",
            "I0408 14:01:01.127345 139805357463424 run_lib.py:133] step: 983000, training_loss: 4.97049e+02\n",
            "I0408 14:01:01.423483 139805357463424 run_lib.py:146] step: 983000, eval_loss: 1.17267e+03\n",
            "I0408 14:01:39.148434 139805357463424 run_lib.py:133] step: 983050, training_loss: 5.64955e+02\n",
            "I0408 14:02:16.874502 139805357463424 run_lib.py:133] step: 983100, training_loss: 8.88858e+02\n",
            "I0408 14:02:17.170478 139805357463424 run_lib.py:146] step: 983100, eval_loss: 6.45383e+02\n",
            "I0408 14:02:54.951354 139805357463424 run_lib.py:133] step: 983150, training_loss: 5.58801e+02\n",
            "I0408 14:03:32.679720 139805357463424 run_lib.py:133] step: 983200, training_loss: 1.51286e+03\n",
            "I0408 14:03:32.975689 139805357463424 run_lib.py:146] step: 983200, eval_loss: 8.00355e+02\n",
            "I0408 14:04:10.708846 139805357463424 run_lib.py:133] step: 983250, training_loss: 1.05275e+03\n",
            "I0408 14:04:48.445994 139805357463424 run_lib.py:133] step: 983300, training_loss: 5.19750e+02\n",
            "I0408 14:04:48.742122 139805357463424 run_lib.py:146] step: 983300, eval_loss: 5.41216e+02\n",
            "I0408 14:05:26.502135 139805357463424 run_lib.py:133] step: 983350, training_loss: 6.13371e+02\n",
            "I0408 14:06:04.260311 139805357463424 run_lib.py:133] step: 983400, training_loss: 5.38780e+02\n",
            "I0408 14:06:04.556457 139805357463424 run_lib.py:146] step: 983400, eval_loss: 9.71020e+02\n",
            "I0408 14:06:42.304717 139805357463424 run_lib.py:133] step: 983450, training_loss: 4.97454e+02\n",
            "I0408 14:07:20.047257 139805357463424 run_lib.py:133] step: 983500, training_loss: 9.39417e+02\n",
            "I0408 14:07:20.344121 139805357463424 run_lib.py:146] step: 983500, eval_loss: 6.01560e+02\n",
            "I0408 14:07:58.104592 139805357463424 run_lib.py:133] step: 983550, training_loss: 4.15079e+02\n",
            "I0408 14:08:35.855025 139805357463424 run_lib.py:133] step: 983600, training_loss: 5.48142e+02\n",
            "I0408 14:08:36.152855 139805357463424 run_lib.py:146] step: 983600, eval_loss: 3.83068e+02\n",
            "I0408 14:09:13.886723 139805357463424 run_lib.py:133] step: 983650, training_loss: 1.12613e+03\n",
            "I0408 14:09:51.626138 139805357463424 run_lib.py:133] step: 983700, training_loss: 7.69320e+02\n",
            "I0408 14:09:51.921716 139805357463424 run_lib.py:146] step: 983700, eval_loss: 5.17527e+02\n",
            "I0408 14:10:29.724727 139805357463424 run_lib.py:133] step: 983750, training_loss: 5.04045e+02\n",
            "I0408 14:11:07.474735 139805357463424 run_lib.py:133] step: 983800, training_loss: 4.94820e+02\n",
            "I0408 14:11:07.770574 139805357463424 run_lib.py:146] step: 983800, eval_loss: 3.59516e+02\n",
            "I0408 14:11:45.513123 139805357463424 run_lib.py:133] step: 983850, training_loss: 1.25893e+03\n",
            "I0408 14:12:23.247273 139805357463424 run_lib.py:133] step: 983900, training_loss: 4.21712e+02\n",
            "I0408 14:12:23.545046 139805357463424 run_lib.py:146] step: 983900, eval_loss: 1.57966e+02\n",
            "I0408 14:13:01.328054 139805357463424 run_lib.py:133] step: 983950, training_loss: 2.66652e+02\n",
            "I0408 14:13:39.118401 139805357463424 run_lib.py:133] step: 984000, training_loss: 4.17242e+02\n",
            "I0408 14:13:39.413736 139805357463424 run_lib.py:146] step: 984000, eval_loss: 4.07135e+02\n",
            "I0408 14:14:17.145956 139805357463424 run_lib.py:133] step: 984050, training_loss: 3.58218e+02\n",
            "I0408 14:14:54.856645 139805357463424 run_lib.py:133] step: 984100, training_loss: 3.05207e+02\n",
            "I0408 14:14:55.153357 139805357463424 run_lib.py:146] step: 984100, eval_loss: 4.28481e+02\n",
            "I0408 14:15:32.934386 139805357463424 run_lib.py:133] step: 984150, training_loss: 7.31496e+02\n",
            "I0408 14:16:10.703831 139805357463424 run_lib.py:133] step: 984200, training_loss: 9.93213e+02\n",
            "I0408 14:16:10.999429 139805357463424 run_lib.py:146] step: 984200, eval_loss: 4.36346e+02\n",
            "I0408 14:16:48.744549 139805357463424 run_lib.py:133] step: 984250, training_loss: 1.02462e+03\n",
            "I0408 14:17:26.486233 139805357463424 run_lib.py:133] step: 984300, training_loss: 6.24339e+02\n",
            "I0408 14:17:26.785913 139805357463424 run_lib.py:146] step: 984300, eval_loss: 3.96088e+02\n",
            "I0408 14:18:04.565805 139805357463424 run_lib.py:133] step: 984350, training_loss: 6.58067e+02\n",
            "I0408 14:18:42.328705 139805357463424 run_lib.py:133] step: 984400, training_loss: 3.97714e+02\n",
            "I0408 14:18:42.624807 139805357463424 run_lib.py:146] step: 984400, eval_loss: 9.62674e+02\n",
            "I0408 14:19:20.394875 139805357463424 run_lib.py:133] step: 984450, training_loss: 7.11601e+02\n",
            "I0408 14:19:58.123878 139805357463424 run_lib.py:133] step: 984500, training_loss: 3.63785e+02\n",
            "I0408 14:19:58.420436 139805357463424 run_lib.py:146] step: 984500, eval_loss: 9.01179e+02\n",
            "I0408 14:20:36.227811 139805357463424 run_lib.py:133] step: 984550, training_loss: 4.37511e+02\n",
            "I0408 14:21:13.982742 139805357463424 run_lib.py:133] step: 984600, training_loss: 4.13984e+02\n",
            "I0408 14:21:14.282079 139805357463424 run_lib.py:146] step: 984600, eval_loss: 4.63802e+02\n",
            "I0408 14:21:52.032268 139805357463424 run_lib.py:133] step: 984650, training_loss: 1.73954e+02\n",
            "I0408 14:22:29.785752 139805357463424 run_lib.py:133] step: 984700, training_loss: 3.92245e+02\n",
            "I0408 14:22:30.081831 139805357463424 run_lib.py:146] step: 984700, eval_loss: 4.31311e+02\n",
            "I0408 14:23:07.895901 139805357463424 run_lib.py:133] step: 984750, training_loss: 3.15940e+02\n",
            "I0408 14:23:45.651041 139805357463424 run_lib.py:133] step: 984800, training_loss: 9.52622e+02\n",
            "I0408 14:23:45.946870 139805357463424 run_lib.py:146] step: 984800, eval_loss: 5.48547e+02\n",
            "I0408 14:24:23.679302 139805357463424 run_lib.py:133] step: 984850, training_loss: 2.21806e+02\n",
            "I0408 14:25:01.410311 139805357463424 run_lib.py:133] step: 984900, training_loss: 3.90391e+02\n",
            "I0408 14:25:01.705686 139805357463424 run_lib.py:146] step: 984900, eval_loss: 7.12207e+02\n",
            "I0408 14:25:39.465291 139805357463424 run_lib.py:133] step: 984950, training_loss: 8.40049e+02\n",
            "I0408 14:26:17.260890 139805357463424 run_lib.py:133] step: 985000, training_loss: 1.11148e+03\n",
            "I0408 14:26:21.751755 139805357463424 run_lib.py:146] step: 985000, eval_loss: 8.60985e+02\n",
            "I0408 14:26:59.886901 139805357463424 run_lib.py:133] step: 985050, training_loss: 5.75430e+02\n",
            "I0408 14:27:37.647764 139805357463424 run_lib.py:133] step: 985100, training_loss: 8.23863e+02\n",
            "I0408 14:27:37.946308 139805357463424 run_lib.py:146] step: 985100, eval_loss: 4.71921e+02\n",
            "I0408 14:28:15.726058 139805357463424 run_lib.py:133] step: 985150, training_loss: 5.10931e+02\n",
            "I0408 14:28:53.497446 139805357463424 run_lib.py:133] step: 985200, training_loss: 1.16996e+03\n",
            "I0408 14:28:53.793812 139805357463424 run_lib.py:146] step: 985200, eval_loss: 8.22336e+02\n",
            "I0408 14:29:31.543386 139805357463424 run_lib.py:133] step: 985250, training_loss: 8.44463e+02\n",
            "I0408 14:30:09.279201 139805357463424 run_lib.py:133] step: 985300, training_loss: 2.47721e+02\n",
            "I0408 14:30:09.574344 139805357463424 run_lib.py:146] step: 985300, eval_loss: 4.00784e+02\n",
            "I0408 14:30:47.303466 139805357463424 run_lib.py:133] step: 985350, training_loss: 1.80163e+03\n",
            "I0408 14:31:25.071855 139805357463424 run_lib.py:133] step: 985400, training_loss: 5.65035e+02\n",
            "I0408 14:31:25.370131 139805357463424 run_lib.py:146] step: 985400, eval_loss: 7.06197e+02\n",
            "I0408 14:32:03.083927 139805357463424 run_lib.py:133] step: 985450, training_loss: 5.73009e+02\n",
            "I0408 14:32:40.850568 139805357463424 run_lib.py:133] step: 985500, training_loss: 5.55690e+02\n",
            "I0408 14:32:41.146589 139805357463424 run_lib.py:146] step: 985500, eval_loss: 5.30545e+02\n",
            "I0408 14:33:18.903140 139805357463424 run_lib.py:133] step: 985550, training_loss: 2.93726e+02\n",
            "I0408 14:33:56.649403 139805357463424 run_lib.py:133] step: 985600, training_loss: 6.47252e+02\n",
            "I0408 14:33:56.946428 139805357463424 run_lib.py:146] step: 985600, eval_loss: 4.65787e+02\n",
            "I0408 14:34:34.681395 139805357463424 run_lib.py:133] step: 985650, training_loss: 3.23475e+02\n",
            "I0408 14:35:12.389698 139805357463424 run_lib.py:133] step: 985700, training_loss: 5.15124e+02\n",
            "I0408 14:35:12.686478 139805357463424 run_lib.py:146] step: 985700, eval_loss: 5.54777e+02\n",
            "I0408 14:35:50.451710 139805357463424 run_lib.py:133] step: 985750, training_loss: 2.69807e+02\n",
            "I0408 14:36:28.268391 139805357463424 run_lib.py:133] step: 985800, training_loss: 6.26142e+02\n",
            "I0408 14:36:28.564195 139805357463424 run_lib.py:146] step: 985800, eval_loss: 6.23535e+02\n",
            "I0408 14:37:06.297806 139805357463424 run_lib.py:133] step: 985850, training_loss: 1.52981e+02\n",
            "I0408 14:37:44.033474 139805357463424 run_lib.py:133] step: 985900, training_loss: 6.83726e+02\n",
            "I0408 14:37:44.329387 139805357463424 run_lib.py:146] step: 985900, eval_loss: 7.53818e+02\n",
            "I0408 14:38:22.113413 139805357463424 run_lib.py:133] step: 985950, training_loss: 4.46066e+02\n",
            "I0408 14:38:59.867763 139805357463424 run_lib.py:133] step: 986000, training_loss: 7.71399e+02\n",
            "I0408 14:39:00.163876 139805357463424 run_lib.py:146] step: 986000, eval_loss: 6.97434e+02\n",
            "I0408 14:39:37.887387 139805357463424 run_lib.py:133] step: 986050, training_loss: 1.11171e+03\n",
            "I0408 14:40:15.612051 139805357463424 run_lib.py:133] step: 986100, training_loss: 3.85216e+02\n",
            "I0408 14:40:15.909233 139805357463424 run_lib.py:146] step: 986100, eval_loss: 3.44358e+02\n",
            "I0408 14:40:53.680060 139805357463424 run_lib.py:133] step: 986150, training_loss: 2.32974e+02\n",
            "I0408 14:41:31.439139 139805357463424 run_lib.py:133] step: 986200, training_loss: 2.24065e+02\n",
            "I0408 14:41:31.735767 139805357463424 run_lib.py:146] step: 986200, eval_loss: 9.27485e+02\n",
            "I0408 14:42:09.519153 139805357463424 run_lib.py:133] step: 986250, training_loss: 5.54160e+02\n",
            "I0408 14:42:47.258676 139805357463424 run_lib.py:133] step: 986300, training_loss: 5.47125e+02\n",
            "I0408 14:42:47.555926 139805357463424 run_lib.py:146] step: 986300, eval_loss: 7.86323e+02\n",
            "I0408 14:43:25.323730 139805357463424 run_lib.py:133] step: 986350, training_loss: 1.12172e+03\n",
            "I0408 14:44:03.049962 139805357463424 run_lib.py:133] step: 986400, training_loss: 6.30045e+02\n",
            "I0408 14:44:03.346033 139805357463424 run_lib.py:146] step: 986400, eval_loss: 3.43500e+02\n",
            "I0408 14:44:41.094770 139805357463424 run_lib.py:133] step: 986450, training_loss: 6.81192e+02\n",
            "I0408 14:45:18.819793 139805357463424 run_lib.py:133] step: 986500, training_loss: 4.34601e+02\n",
            "I0408 14:45:19.115590 139805357463424 run_lib.py:146] step: 986500, eval_loss: 3.38297e+02\n",
            "I0408 14:45:56.855318 139805357463424 run_lib.py:133] step: 986550, training_loss: 8.36445e+02\n",
            "I0408 14:46:34.646175 139805357463424 run_lib.py:133] step: 986600, training_loss: 6.72843e+02\n",
            "I0408 14:46:34.941739 139805357463424 run_lib.py:146] step: 986600, eval_loss: 2.86373e+02\n",
            "I0408 14:47:12.677508 139805357463424 run_lib.py:133] step: 986650, training_loss: 7.19075e+02\n",
            "I0408 14:47:50.423403 139805357463424 run_lib.py:133] step: 986700, training_loss: 1.14197e+02\n",
            "I0408 14:47:50.718907 139805357463424 run_lib.py:146] step: 986700, eval_loss: 7.38954e+02\n",
            "I0408 14:48:28.470411 139805357463424 run_lib.py:133] step: 986750, training_loss: 6.83122e+02\n",
            "I0408 14:49:06.202100 139805357463424 run_lib.py:133] step: 986800, training_loss: 8.88095e+02\n",
            "I0408 14:49:06.498634 139805357463424 run_lib.py:146] step: 986800, eval_loss: 1.85589e+02\n",
            "I0408 14:49:44.215528 139805357463424 run_lib.py:133] step: 986850, training_loss: 7.80542e+02\n",
            "I0408 14:50:21.955218 139805357463424 run_lib.py:133] step: 986900, training_loss: 3.31324e+02\n",
            "I0408 14:50:22.251920 139805357463424 run_lib.py:146] step: 986900, eval_loss: 3.19294e+02\n",
            "I0408 14:50:59.997194 139805357463424 run_lib.py:133] step: 986950, training_loss: 6.71341e+02\n",
            "I0408 14:51:37.767828 139805357463424 run_lib.py:133] step: 987000, training_loss: 2.44149e+02\n",
            "I0408 14:51:38.064896 139805357463424 run_lib.py:146] step: 987000, eval_loss: 5.34507e+02\n",
            "I0408 14:52:15.801775 139805357463424 run_lib.py:133] step: 987050, training_loss: 6.57564e+02\n",
            "I0408 14:52:53.512200 139805357463424 run_lib.py:133] step: 987100, training_loss: 2.42729e+02\n",
            "I0408 14:52:53.808662 139805357463424 run_lib.py:146] step: 987100, eval_loss: 6.22750e+02\n",
            "I0408 14:53:31.582592 139805357463424 run_lib.py:133] step: 987150, training_loss: 6.72921e+02\n",
            "I0408 14:54:09.317286 139805357463424 run_lib.py:133] step: 987200, training_loss: 4.75347e+02\n",
            "I0408 14:54:09.617496 139805357463424 run_lib.py:146] step: 987200, eval_loss: 4.18987e+02\n",
            "I0408 14:54:47.336819 139805357463424 run_lib.py:133] step: 987250, training_loss: 6.32426e+02\n",
            "I0408 14:55:25.049071 139805357463424 run_lib.py:133] step: 987300, training_loss: 5.82289e+02\n",
            "I0408 14:55:25.345524 139805357463424 run_lib.py:146] step: 987300, eval_loss: 2.75740e+02\n",
            "I0408 14:56:03.115744 139805357463424 run_lib.py:133] step: 987350, training_loss: 6.35576e+02\n",
            "I0408 14:56:40.900470 139805357463424 run_lib.py:133] step: 987400, training_loss: 7.01234e+02\n",
            "I0408 14:56:41.197498 139805357463424 run_lib.py:146] step: 987400, eval_loss: 3.09143e+02\n",
            "I0408 14:57:18.965595 139805357463424 run_lib.py:133] step: 987450, training_loss: 3.64286e+02\n",
            "I0408 14:57:56.666787 139805357463424 run_lib.py:133] step: 987500, training_loss: 4.88820e+02\n",
            "I0408 14:57:56.963260 139805357463424 run_lib.py:146] step: 987500, eval_loss: 9.98166e+02\n",
            "I0408 14:58:34.732775 139805357463424 run_lib.py:133] step: 987550, training_loss: 7.33559e+02\n",
            "I0408 14:59:12.474760 139805357463424 run_lib.py:133] step: 987600, training_loss: 3.27092e+02\n",
            "I0408 14:59:12.771277 139805357463424 run_lib.py:146] step: 987600, eval_loss: 8.40813e+02\n",
            "I0408 14:59:50.506386 139805357463424 run_lib.py:133] step: 987650, training_loss: 3.18457e+02\n",
            "I0408 15:00:28.220529 139805357463424 run_lib.py:133] step: 987700, training_loss: 6.68807e+02\n",
            "I0408 15:00:28.516072 139805357463424 run_lib.py:146] step: 987700, eval_loss: 4.70345e+02\n",
            "I0408 15:01:06.236651 139805357463424 run_lib.py:133] step: 987750, training_loss: 1.15601e+03\n",
            "I0408 15:01:43.975648 139805357463424 run_lib.py:133] step: 987800, training_loss: 6.62922e+02\n",
            "I0408 15:01:44.272209 139805357463424 run_lib.py:146] step: 987800, eval_loss: 3.25389e+02\n",
            "I0408 15:02:22.041457 139805357463424 run_lib.py:133] step: 987850, training_loss: 5.11973e+02\n",
            "I0408 15:02:59.753351 139805357463424 run_lib.py:133] step: 987900, training_loss: 1.09132e+03\n",
            "I0408 15:03:00.048599 139805357463424 run_lib.py:146] step: 987900, eval_loss: 6.90876e+02\n",
            "I0408 15:03:37.786421 139805357463424 run_lib.py:133] step: 987950, training_loss: 3.14923e+02\n",
            "I0408 15:04:15.531508 139805357463424 run_lib.py:133] step: 988000, training_loss: 2.08520e+02\n",
            "I0408 15:04:15.830344 139805357463424 run_lib.py:146] step: 988000, eval_loss: 5.49854e+02\n",
            "I0408 15:04:53.519909 139805357463424 run_lib.py:133] step: 988050, training_loss: 2.63565e+02\n",
            "I0408 15:05:31.246308 139805357463424 run_lib.py:133] step: 988100, training_loss: 3.86426e+02\n",
            "I0408 15:05:31.542489 139805357463424 run_lib.py:146] step: 988100, eval_loss: 1.58280e+02\n",
            "I0408 15:06:09.283397 139805357463424 run_lib.py:133] step: 988150, training_loss: 6.90672e+02\n",
            "I0408 15:06:47.019260 139805357463424 run_lib.py:133] step: 988200, training_loss: 5.10167e+02\n",
            "I0408 15:06:47.314314 139805357463424 run_lib.py:146] step: 988200, eval_loss: 6.53025e+02\n",
            "I0408 15:07:25.108140 139805357463424 run_lib.py:133] step: 988250, training_loss: 8.89245e+02\n",
            "I0408 15:08:02.992778 139805357463424 run_lib.py:133] step: 988300, training_loss: 4.97796e+02\n",
            "I0408 15:08:03.290491 139805357463424 run_lib.py:146] step: 988300, eval_loss: 5.04004e+02\n",
            "I0408 15:08:41.088688 139805357463424 run_lib.py:133] step: 988350, training_loss: 4.25563e+02\n",
            "I0408 15:09:18.859901 139805357463424 run_lib.py:133] step: 988400, training_loss: 6.60718e+02\n",
            "I0408 15:09:19.155579 139805357463424 run_lib.py:146] step: 988400, eval_loss: 9.99980e+02\n",
            "I0408 15:09:56.904850 139805357463424 run_lib.py:133] step: 988450, training_loss: 6.48180e+02\n",
            "I0408 15:10:34.628958 139805357463424 run_lib.py:133] step: 988500, training_loss: 5.45591e+02\n",
            "I0408 15:10:34.925865 139805357463424 run_lib.py:146] step: 988500, eval_loss: 1.22841e+03\n",
            "I0408 15:11:12.684253 139805357463424 run_lib.py:133] step: 988550, training_loss: 1.12823e+03\n",
            "I0408 15:11:50.465927 139805357463424 run_lib.py:133] step: 988600, training_loss: 8.53280e+02\n",
            "I0408 15:11:50.763005 139805357463424 run_lib.py:146] step: 988600, eval_loss: 1.85086e+02\n",
            "I0408 15:12:28.531605 139805357463424 run_lib.py:133] step: 988650, training_loss: 9.31681e+02\n",
            "I0408 15:13:06.279315 139805357463424 run_lib.py:133] step: 988700, training_loss: 3.44504e+02\n",
            "I0408 15:13:06.576832 139805357463424 run_lib.py:146] step: 988700, eval_loss: 8.30049e+02\n",
            "I0408 15:13:44.271158 139805357463424 run_lib.py:133] step: 988750, training_loss: 9.99164e+02\n",
            "I0408 15:14:21.992019 139805357463424 run_lib.py:133] step: 988800, training_loss: 1.14672e+02\n",
            "I0408 15:14:22.287204 139805357463424 run_lib.py:146] step: 988800, eval_loss: 1.13377e+03\n",
            "I0408 15:14:59.982670 139805357463424 run_lib.py:133] step: 988850, training_loss: 3.56977e+02\n",
            "I0408 15:15:37.652843 139805357463424 run_lib.py:133] step: 988900, training_loss: 3.31645e+02\n",
            "I0408 15:15:37.948828 139805357463424 run_lib.py:146] step: 988900, eval_loss: 9.12946e+02\n",
            "I0408 15:16:15.625593 139805357463424 run_lib.py:133] step: 988950, training_loss: 9.63317e+02\n",
            "I0408 15:16:53.338948 139805357463424 run_lib.py:133] step: 989000, training_loss: 7.81399e+02\n",
            "I0408 15:16:53.640005 139805357463424 run_lib.py:146] step: 989000, eval_loss: 3.83060e+02\n",
            "I0408 15:17:31.424221 139805357463424 run_lib.py:133] step: 989050, training_loss: 2.44243e+02\n",
            "I0408 15:18:09.203320 139805357463424 run_lib.py:133] step: 989100, training_loss: 6.39426e+02\n",
            "I0408 15:18:09.500205 139805357463424 run_lib.py:146] step: 989100, eval_loss: 5.32285e+02\n",
            "I0408 15:18:47.210860 139805357463424 run_lib.py:133] step: 989150, training_loss: 6.60833e+02\n",
            "I0408 15:19:24.898386 139805357463424 run_lib.py:133] step: 989200, training_loss: 7.76909e+02\n",
            "I0408 15:19:25.193846 139805357463424 run_lib.py:146] step: 989200, eval_loss: 1.25832e+03\n",
            "I0408 15:20:02.872071 139805357463424 run_lib.py:133] step: 989250, training_loss: 2.49533e+02\n",
            "I0408 15:20:40.575927 139805357463424 run_lib.py:133] step: 989300, training_loss: 3.70215e+02\n",
            "I0408 15:20:40.872045 139805357463424 run_lib.py:146] step: 989300, eval_loss: 3.99820e+02\n",
            "I0408 15:21:18.566401 139805357463424 run_lib.py:133] step: 989350, training_loss: 8.44302e+02\n",
            "I0408 15:21:56.263664 139805357463424 run_lib.py:133] step: 989400, training_loss: 5.69875e+02\n",
            "I0408 15:21:56.558885 139805357463424 run_lib.py:146] step: 989400, eval_loss: 9.42913e+02\n",
            "I0408 15:22:34.243943 139805357463424 run_lib.py:133] step: 989450, training_loss: 5.14832e+02\n",
            "I0408 15:23:11.933206 139805357463424 run_lib.py:133] step: 989500, training_loss: 5.13424e+02\n",
            "I0408 15:23:12.228244 139805357463424 run_lib.py:146] step: 989500, eval_loss: 4.73687e+02\n",
            "I0408 15:23:49.895403 139805357463424 run_lib.py:133] step: 989550, training_loss: 1.19089e+03\n",
            "I0408 15:24:27.584136 139805357463424 run_lib.py:133] step: 989600, training_loss: 4.59212e+02\n",
            "I0408 15:24:27.883123 139805357463424 run_lib.py:146] step: 989600, eval_loss: 9.22613e+02\n",
            "I0408 15:25:05.576723 139805357463424 run_lib.py:133] step: 989650, training_loss: 2.41829e+02\n",
            "I0408 15:25:43.375709 139805357463424 run_lib.py:133] step: 989700, training_loss: 7.43719e+02\n",
            "I0408 15:25:43.679378 139805357463424 run_lib.py:146] step: 989700, eval_loss: 6.18472e+02\n",
            "I0408 15:26:21.594287 139805357463424 run_lib.py:133] step: 989750, training_loss: 8.62638e+02\n",
            "I0408 15:26:59.473814 139805357463424 run_lib.py:133] step: 989800, training_loss: 7.94240e+02\n",
            "I0408 15:26:59.772209 139805357463424 run_lib.py:146] step: 989800, eval_loss: 5.03463e+02\n",
            "I0408 15:27:37.691639 139805357463424 run_lib.py:133] step: 989850, training_loss: 7.99254e+02\n",
            "I0408 15:28:15.606737 139805357463424 run_lib.py:133] step: 989900, training_loss: 3.55821e+02\n",
            "I0408 15:28:15.906728 139805357463424 run_lib.py:146] step: 989900, eval_loss: 8.71337e+02\n",
            "I0408 15:28:53.790622 139805357463424 run_lib.py:133] step: 989950, training_loss: 3.47088e+02\n",
            "I0408 15:29:31.679964 139805357463424 run_lib.py:133] step: 990000, training_loss: 4.57518e+02\n",
            "I0408 15:29:36.968643 139805357463424 run_lib.py:146] step: 990000, eval_loss: 5.64177e+02\n",
            "I0408 15:30:15.042393 139805357463424 run_lib.py:133] step: 990050, training_loss: 2.70247e+02\n",
            "I0408 15:30:52.911714 139805357463424 run_lib.py:133] step: 990100, training_loss: 4.58417e+02\n",
            "I0408 15:30:53.209844 139805357463424 run_lib.py:146] step: 990100, eval_loss: 5.13900e+02\n",
            "I0408 15:31:31.071982 139805357463424 run_lib.py:133] step: 990150, training_loss: 8.51336e+02\n",
            "I0408 15:32:08.895201 139805357463424 run_lib.py:133] step: 990200, training_loss: 6.27381e+02\n",
            "I0408 15:32:09.194631 139805357463424 run_lib.py:146] step: 990200, eval_loss: 8.91662e+02\n",
            "I0408 15:32:47.040232 139805357463424 run_lib.py:133] step: 990250, training_loss: 7.20812e+02\n",
            "I0408 15:33:24.921265 139805357463424 run_lib.py:133] step: 990300, training_loss: 4.40196e+02\n",
            "I0408 15:33:25.222431 139805357463424 run_lib.py:146] step: 990300, eval_loss: 8.08423e+02\n",
            "I0408 15:34:03.085422 139805357463424 run_lib.py:133] step: 990350, training_loss: 2.22171e+02\n",
            "I0408 15:34:40.961435 139805357463424 run_lib.py:133] step: 990400, training_loss: 4.92637e+02\n",
            "I0408 15:34:41.258423 139805357463424 run_lib.py:146] step: 990400, eval_loss: 3.00191e+02\n",
            "I0408 15:35:19.101348 139805357463424 run_lib.py:133] step: 990450, training_loss: 7.73218e+02\n",
            "I0408 15:35:56.868107 139805357463424 run_lib.py:133] step: 990500, training_loss: 2.40547e+02\n",
            "I0408 15:35:57.163994 139805357463424 run_lib.py:146] step: 990500, eval_loss: 7.32936e+02\n",
            "I0408 15:36:34.894336 139805357463424 run_lib.py:133] step: 990550, training_loss: 4.50038e+02\n",
            "I0408 15:37:12.641212 139805357463424 run_lib.py:133] step: 990600, training_loss: 3.22413e+02\n",
            "I0408 15:37:12.938195 139805357463424 run_lib.py:146] step: 990600, eval_loss: 5.42755e+02\n",
            "I0408 15:37:50.727033 139805357463424 run_lib.py:133] step: 990650, training_loss: 4.56027e+02\n",
            "I0408 15:38:28.486984 139805357463424 run_lib.py:133] step: 990700, training_loss: 3.10661e+02\n",
            "I0408 15:38:28.783463 139805357463424 run_lib.py:146] step: 990700, eval_loss: 8.06484e+02\n",
            "I0408 15:39:06.548488 139805357463424 run_lib.py:133] step: 990750, training_loss: 6.13701e+02\n",
            "I0408 15:39:44.290050 139805357463424 run_lib.py:133] step: 990800, training_loss: 4.63294e+02\n",
            "I0408 15:39:44.586110 139805357463424 run_lib.py:146] step: 990800, eval_loss: 8.02994e+02\n",
            "I0408 15:40:22.376083 139805357463424 run_lib.py:133] step: 990850, training_loss: 1.03386e+03\n",
            "I0408 15:41:00.203927 139805357463424 run_lib.py:133] step: 990900, training_loss: 5.75149e+02\n",
            "I0408 15:41:00.501532 139805357463424 run_lib.py:146] step: 990900, eval_loss: 3.25693e+02\n",
            "I0408 15:41:38.330760 139805357463424 run_lib.py:133] step: 990950, training_loss: 7.97866e+02\n",
            "I0408 15:42:16.245282 139805357463424 run_lib.py:133] step: 991000, training_loss: 1.01298e+03\n",
            "I0408 15:42:16.546619 139805357463424 run_lib.py:146] step: 991000, eval_loss: 5.16197e+02\n",
            "I0408 15:42:54.463543 139805357463424 run_lib.py:133] step: 991050, training_loss: 3.49723e+02\n",
            "I0408 15:43:32.370963 139805357463424 run_lib.py:133] step: 991100, training_loss: 1.16714e+03\n",
            "I0408 15:43:32.670037 139805357463424 run_lib.py:146] step: 991100, eval_loss: 6.16054e+02\n",
            "I0408 15:44:10.591717 139805357463424 run_lib.py:133] step: 991150, training_loss: 8.16881e+02\n",
            "I0408 15:44:48.469046 139805357463424 run_lib.py:133] step: 991200, training_loss: 1.73318e+02\n",
            "I0408 15:44:48.768080 139805357463424 run_lib.py:146] step: 991200, eval_loss: 6.40671e+02\n",
            "I0408 15:45:26.677634 139805357463424 run_lib.py:133] step: 991250, training_loss: 6.41691e+02\n",
            "I0408 15:46:04.579626 139805357463424 run_lib.py:133] step: 991300, training_loss: 2.42162e+02\n",
            "I0408 15:46:04.878767 139805357463424 run_lib.py:146] step: 991300, eval_loss: 6.37015e+02\n",
            "I0408 15:46:42.801340 139805357463424 run_lib.py:133] step: 991350, training_loss: 4.79262e+02\n",
            "I0408 15:47:20.700626 139805357463424 run_lib.py:133] step: 991400, training_loss: 6.28674e+02\n",
            "I0408 15:47:20.999435 139805357463424 run_lib.py:146] step: 991400, eval_loss: 9.02479e+02\n",
            "I0408 15:47:58.891586 139805357463424 run_lib.py:133] step: 991450, training_loss: 6.39456e+02\n",
            "I0408 15:48:36.789415 139805357463424 run_lib.py:133] step: 991500, training_loss: 8.18729e+02\n",
            "I0408 15:48:37.089058 139805357463424 run_lib.py:146] step: 991500, eval_loss: 6.72410e+02\n",
            "I0408 15:49:14.996928 139805357463424 run_lib.py:133] step: 991550, training_loss: 6.67016e+02\n",
            "I0408 15:49:52.908195 139805357463424 run_lib.py:133] step: 991600, training_loss: 6.45063e+02\n",
            "I0408 15:49:53.207052 139805357463424 run_lib.py:146] step: 991600, eval_loss: 1.26054e+02\n",
            "I0408 15:50:31.116002 139805357463424 run_lib.py:133] step: 991650, training_loss: 8.29526e+02\n",
            "I0408 15:51:09.006285 139805357463424 run_lib.py:133] step: 991700, training_loss: 5.14109e+02\n",
            "I0408 15:51:09.305419 139805357463424 run_lib.py:146] step: 991700, eval_loss: 3.07193e+02\n",
            "I0408 15:51:47.240911 139805357463424 run_lib.py:133] step: 991750, training_loss: 8.82148e+02\n",
            "I0408 15:52:25.121577 139805357463424 run_lib.py:133] step: 991800, training_loss: 7.92178e+02\n",
            "I0408 15:52:25.421723 139805357463424 run_lib.py:146] step: 991800, eval_loss: 5.14131e+02\n",
            "I0408 15:53:03.297478 139805357463424 run_lib.py:133] step: 991850, training_loss: 7.90934e+02\n",
            "I0408 15:53:41.195851 139805357463424 run_lib.py:133] step: 991900, training_loss: 4.78392e+02\n",
            "I0408 15:53:41.496802 139805357463424 run_lib.py:146] step: 991900, eval_loss: 8.52526e+02\n",
            "I0408 15:54:19.422015 139805357463424 run_lib.py:133] step: 991950, training_loss: 7.98676e+02\n",
            "I0408 15:54:57.303190 139805357463424 run_lib.py:133] step: 992000, training_loss: 8.43054e+02\n",
            "I0408 15:54:57.601062 139805357463424 run_lib.py:146] step: 992000, eval_loss: 4.79349e+02\n",
            "I0408 15:55:35.426107 139805357463424 run_lib.py:133] step: 992050, training_loss: 1.23735e+03\n",
            "I0408 15:56:13.261272 139805357463424 run_lib.py:133] step: 992100, training_loss: 4.04224e+02\n",
            "I0408 15:56:13.559119 139805357463424 run_lib.py:146] step: 992100, eval_loss: 7.08289e+02\n",
            "I0408 15:56:51.479138 139805357463424 run_lib.py:133] step: 992150, training_loss: 2.82534e+02\n",
            "I0408 15:57:29.368132 139805357463424 run_lib.py:133] step: 992200, training_loss: 3.74316e+02\n",
            "I0408 15:57:29.666575 139805357463424 run_lib.py:146] step: 992200, eval_loss: 8.79958e+02\n",
            "I0408 15:58:07.559924 139805357463424 run_lib.py:133] step: 992250, training_loss: 5.03458e+02\n",
            "I0408 15:58:45.447490 139805357463424 run_lib.py:133] step: 992300, training_loss: 6.54610e+02\n",
            "I0408 15:58:45.746454 139805357463424 run_lib.py:146] step: 992300, eval_loss: 5.25385e+02\n",
            "I0408 15:59:23.671866 139805357463424 run_lib.py:133] step: 992350, training_loss: 6.63550e+02\n",
            "I0408 16:00:01.522098 139805357463424 run_lib.py:133] step: 992400, training_loss: 1.48008e+03\n",
            "I0408 16:00:01.819503 139805357463424 run_lib.py:146] step: 992400, eval_loss: 4.99633e+02\n",
            "I0408 16:00:39.674914 139805357463424 run_lib.py:133] step: 992450, training_loss: 2.16922e+02\n",
            "I0408 16:01:17.515839 139805357463424 run_lib.py:133] step: 992500, training_loss: 5.95829e+02\n",
            "I0408 16:01:17.814510 139805357463424 run_lib.py:146] step: 992500, eval_loss: 6.55481e+02\n",
            "I0408 16:01:55.697175 139805357463424 run_lib.py:133] step: 992550, training_loss: 7.53526e+02\n",
            "I0408 16:02:33.615493 139805357463424 run_lib.py:133] step: 992600, training_loss: 4.10134e+02\n",
            "I0408 16:02:33.913899 139805357463424 run_lib.py:146] step: 992600, eval_loss: 3.38005e+02\n",
            "I0408 16:03:11.863933 139805357463424 run_lib.py:133] step: 992650, training_loss: 3.44491e+02\n",
            "I0408 16:03:49.764214 139805357463424 run_lib.py:133] step: 992700, training_loss: 3.58311e+02\n",
            "I0408 16:03:50.063112 139805357463424 run_lib.py:146] step: 992700, eval_loss: 1.90837e+02\n",
            "I0408 16:04:27.967946 139805357463424 run_lib.py:133] step: 992750, training_loss: 7.22874e+02\n",
            "I0408 16:05:05.819461 139805357463424 run_lib.py:133] step: 992800, training_loss: 7.40902e+02\n",
            "I0408 16:05:06.117203 139805357463424 run_lib.py:146] step: 992800, eval_loss: 4.55917e+02\n",
            "I0408 16:05:43.959453 139805357463424 run_lib.py:133] step: 992850, training_loss: 7.53496e+02\n",
            "I0408 16:06:21.821100 139805357463424 run_lib.py:133] step: 992900, training_loss: 6.35715e+02\n",
            "I0408 16:06:22.120387 139805357463424 run_lib.py:146] step: 992900, eval_loss: 1.20196e+03\n",
            "I0408 16:07:00.019795 139805357463424 run_lib.py:133] step: 992950, training_loss: 4.10607e+02\n",
            "I0408 16:07:37.944440 139805357463424 run_lib.py:133] step: 993000, training_loss: 1.13089e+03\n",
            "I0408 16:07:38.244372 139805357463424 run_lib.py:146] step: 993000, eval_loss: 1.10878e+03\n",
            "I0408 16:08:16.138277 139805357463424 run_lib.py:133] step: 993050, training_loss: 6.27206e+02\n",
            "I0408 16:08:54.053308 139805357463424 run_lib.py:133] step: 993100, training_loss: 5.04499e+02\n",
            "I0408 16:08:54.353658 139805357463424 run_lib.py:146] step: 993100, eval_loss: 7.63930e+02\n",
            "I0408 16:09:32.252653 139805357463424 run_lib.py:133] step: 993150, training_loss: 7.11435e+02\n",
            "I0408 16:10:10.128877 139805357463424 run_lib.py:133] step: 993200, training_loss: 7.00527e+02\n",
            "I0408 16:10:10.428160 139805357463424 run_lib.py:146] step: 993200, eval_loss: 4.62567e+02\n",
            "I0408 16:10:48.269353 139805357463424 run_lib.py:133] step: 993250, training_loss: 4.85128e+02\n",
            "I0408 16:11:26.203512 139805357463424 run_lib.py:133] step: 993300, training_loss: 4.13794e+02\n",
            "I0408 16:11:26.503660 139805357463424 run_lib.py:146] step: 993300, eval_loss: 4.59385e+02\n",
            "I0408 16:12:04.409776 139805357463424 run_lib.py:133] step: 993350, training_loss: 8.73780e+02\n",
            "I0408 16:12:42.323706 139805357463424 run_lib.py:133] step: 993400, training_loss: 4.43064e+02\n",
            "I0408 16:12:42.622149 139805357463424 run_lib.py:146] step: 993400, eval_loss: 7.36572e+02\n",
            "I0408 16:13:20.516452 139805357463424 run_lib.py:133] step: 993450, training_loss: 4.94088e+02\n",
            "I0408 16:13:58.433577 139805357463424 run_lib.py:133] step: 993500, training_loss: 4.26928e+02\n",
            "I0408 16:13:58.733116 139805357463424 run_lib.py:146] step: 993500, eval_loss: 3.07648e+02\n",
            "I0408 16:14:36.630938 139805357463424 run_lib.py:133] step: 993550, training_loss: 6.01163e+02\n",
            "I0408 16:15:14.599642 139805357463424 run_lib.py:133] step: 993600, training_loss: 7.18922e+02\n",
            "I0408 16:15:14.897401 139805357463424 run_lib.py:146] step: 993600, eval_loss: 5.15838e+02\n",
            "I0408 16:15:52.823957 139805357463424 run_lib.py:133] step: 993650, training_loss: 5.75382e+02\n",
            "I0408 16:16:30.731124 139805357463424 run_lib.py:133] step: 993700, training_loss: 2.01399e+02\n",
            "I0408 16:16:31.031584 139805357463424 run_lib.py:146] step: 993700, eval_loss: 3.73830e+02\n",
            "I0408 16:17:08.997619 139805357463424 run_lib.py:133] step: 993750, training_loss: 6.66490e+02\n",
            "I0408 16:17:46.951102 139805357463424 run_lib.py:133] step: 993800, training_loss: 6.07753e+02\n",
            "I0408 16:17:47.250242 139805357463424 run_lib.py:146] step: 993800, eval_loss: 5.69133e+02\n",
            "I0408 16:18:25.159650 139805357463424 run_lib.py:133] step: 993850, training_loss: 3.27084e+02\n",
            "I0408 16:19:03.037339 139805357463424 run_lib.py:133] step: 993900, training_loss: 6.90319e+02\n",
            "I0408 16:19:03.335423 139805357463424 run_lib.py:146] step: 993900, eval_loss: 4.85736e+02\n",
            "I0408 16:19:41.236581 139805357463424 run_lib.py:133] step: 993950, training_loss: 5.22020e+02\n",
            "I0408 16:20:19.134566 139805357463424 run_lib.py:133] step: 994000, training_loss: 7.35387e+02\n",
            "I0408 16:20:19.432385 139805357463424 run_lib.py:146] step: 994000, eval_loss: 2.79233e+02\n",
            "I0408 16:20:57.297596 139805357463424 run_lib.py:133] step: 994050, training_loss: 8.23712e+02\n",
            "I0408 16:21:35.229944 139805357463424 run_lib.py:133] step: 994100, training_loss: 5.57572e+02\n",
            "I0408 16:21:35.531697 139805357463424 run_lib.py:146] step: 994100, eval_loss: 5.28740e+02\n",
            "I0408 16:22:13.450301 139805357463424 run_lib.py:133] step: 994150, training_loss: 8.40347e+02\n",
            "I0408 16:22:51.365743 139805357463424 run_lib.py:133] step: 994200, training_loss: 5.22970e+02\n",
            "I0408 16:22:51.666343 139805357463424 run_lib.py:146] step: 994200, eval_loss: 8.68540e+02\n",
            "I0408 16:23:29.590911 139805357463424 run_lib.py:133] step: 994250, training_loss: 3.74360e+02\n",
            "I0408 16:24:07.493243 139805357463424 run_lib.py:133] step: 994300, training_loss: 8.40122e+02\n",
            "I0408 16:24:07.791429 139805357463424 run_lib.py:146] step: 994300, eval_loss: 3.02932e+02\n",
            "I0408 16:24:45.644677 139805357463424 run_lib.py:133] step: 994350, training_loss: 6.91868e+02\n",
            "I0408 16:25:23.519139 139805357463424 run_lib.py:133] step: 994400, training_loss: 3.41566e+02\n",
            "I0408 16:25:23.820937 139805357463424 run_lib.py:146] step: 994400, eval_loss: 7.63357e+02\n",
            "I0408 16:26:01.692342 139805357463424 run_lib.py:133] step: 994450, training_loss: 5.26130e+02\n",
            "I0408 16:26:39.628745 139805357463424 run_lib.py:133] step: 994500, training_loss: 1.36284e+03\n",
            "I0408 16:26:39.928184 139805357463424 run_lib.py:146] step: 994500, eval_loss: 4.94535e+02\n",
            "I0408 16:27:17.849989 139805357463424 run_lib.py:133] step: 994550, training_loss: 9.80610e+02\n",
            "I0408 16:27:55.748940 139805357463424 run_lib.py:133] step: 994600, training_loss: 6.06882e+02\n",
            "I0408 16:27:56.049544 139805357463424 run_lib.py:146] step: 994600, eval_loss: 2.67806e+02\n",
            "I0408 16:28:33.959147 139805357463424 run_lib.py:133] step: 994650, training_loss: 7.77101e+02\n",
            "I0408 16:29:11.820387 139805357463424 run_lib.py:133] step: 994700, training_loss: 8.60968e+02\n",
            "I0408 16:29:12.119494 139805357463424 run_lib.py:146] step: 994700, eval_loss: 2.28849e+02\n",
            "I0408 16:29:49.993327 139805357463424 run_lib.py:133] step: 994750, training_loss: 5.26913e+02\n",
            "I0408 16:30:27.863456 139805357463424 run_lib.py:133] step: 994800, training_loss: 3.79436e+02\n",
            "I0408 16:30:28.163475 139805357463424 run_lib.py:146] step: 994800, eval_loss: 4.25832e+02\n",
            "I0408 16:31:06.049646 139805357463424 run_lib.py:133] step: 994850, training_loss: 6.47810e+02\n",
            "I0408 16:31:43.933990 139805357463424 run_lib.py:133] step: 994900, training_loss: 3.75315e+02\n",
            "I0408 16:31:44.232896 139805357463424 run_lib.py:146] step: 994900, eval_loss: 6.45518e+02\n",
            "I0408 16:32:22.164176 139805357463424 run_lib.py:133] step: 994950, training_loss: 6.07145e+02\n",
            "I0408 16:33:00.074864 139805357463424 run_lib.py:133] step: 995000, training_loss: 5.38047e+02\n",
            "I0408 16:33:04.318204 139805357463424 run_lib.py:146] step: 995000, eval_loss: 4.00550e+02\n",
            "I0408 16:33:42.581331 139805357463424 run_lib.py:133] step: 995050, training_loss: 3.74309e+02\n",
            "I0408 16:34:20.453286 139805357463424 run_lib.py:133] step: 995100, training_loss: 5.10351e+02\n",
            "I0408 16:34:20.752397 139805357463424 run_lib.py:146] step: 995100, eval_loss: 7.14798e+02\n",
            "I0408 16:34:58.614118 139805357463424 run_lib.py:133] step: 995150, training_loss: 6.61788e+02\n",
            "I0408 16:35:36.515758 139805357463424 run_lib.py:133] step: 995200, training_loss: 6.67068e+02\n",
            "I0408 16:35:36.816010 139805357463424 run_lib.py:146] step: 995200, eval_loss: 4.95636e+02\n",
            "I0408 16:36:14.741778 139805357463424 run_lib.py:133] step: 995250, training_loss: 5.54273e+02\n",
            "I0408 16:36:52.661486 139805357463424 run_lib.py:133] step: 995300, training_loss: 3.63407e+02\n",
            "I0408 16:36:52.960246 139805357463424 run_lib.py:146] step: 995300, eval_loss: 3.18399e+02\n",
            "I0408 16:37:30.882082 139805357463424 run_lib.py:133] step: 995350, training_loss: 1.02018e+03\n",
            "I0408 16:38:08.779541 139805357463424 run_lib.py:133] step: 995400, training_loss: 4.58300e+02\n",
            "I0408 16:38:09.077846 139805357463424 run_lib.py:146] step: 995400, eval_loss: 6.34117e+02\n",
            "I0408 16:38:46.962405 139805357463424 run_lib.py:133] step: 995450, training_loss: 8.24005e+02\n",
            "I0408 16:39:24.813878 139805357463424 run_lib.py:133] step: 995500, training_loss: 9.79795e+02\n",
            "I0408 16:39:25.113309 139805357463424 run_lib.py:146] step: 995500, eval_loss: 7.14167e+02\n",
            "I0408 16:40:02.989631 139805357463424 run_lib.py:133] step: 995550, training_loss: 4.12661e+02\n",
            "I0408 16:40:40.906341 139805357463424 run_lib.py:133] step: 995600, training_loss: 7.31142e+02\n",
            "I0408 16:40:41.205705 139805357463424 run_lib.py:146] step: 995600, eval_loss: 5.37706e+02\n",
            "I0408 16:41:19.073465 139805357463424 run_lib.py:133] step: 995650, training_loss: 5.80620e+02\n",
            "I0408 16:41:56.923979 139805357463424 run_lib.py:133] step: 995700, training_loss: 4.12379e+02\n",
            "I0408 16:41:57.221738 139805357463424 run_lib.py:146] step: 995700, eval_loss: 5.30713e+02\n",
            "I0408 16:42:35.110075 139805357463424 run_lib.py:133] step: 995750, training_loss: 5.66236e+02\n",
            "I0408 16:43:12.965802 139805357463424 run_lib.py:133] step: 995800, training_loss: 1.02137e+03\n",
            "I0408 16:43:13.264049 139805357463424 run_lib.py:146] step: 995800, eval_loss: 1.01252e+03\n",
            "I0408 16:43:51.107997 139805357463424 run_lib.py:133] step: 995850, training_loss: 1.09484e+03\n",
            "I0408 16:44:28.981892 139805357463424 run_lib.py:133] step: 995900, training_loss: 5.91955e+02\n",
            "I0408 16:44:29.280604 139805357463424 run_lib.py:146] step: 995900, eval_loss: 6.26647e+02\n",
            "I0408 16:45:07.121817 139805357463424 run_lib.py:133] step: 995950, training_loss: 1.34713e+02\n",
            "I0408 16:45:44.963794 139805357463424 run_lib.py:133] step: 996000, training_loss: 7.70306e+02\n",
            "I0408 16:45:45.261892 139805357463424 run_lib.py:146] step: 996000, eval_loss: 4.46711e+02\n",
            "I0408 16:46:23.105997 139805357463424 run_lib.py:133] step: 996050, training_loss: 1.17432e+03\n",
            "I0408 16:47:00.984193 139805357463424 run_lib.py:133] step: 996100, training_loss: 6.82524e+02\n",
            "I0408 16:47:01.283296 139805357463424 run_lib.py:146] step: 996100, eval_loss: 8.96015e+02\n",
            "I0408 16:47:39.122400 139805357463424 run_lib.py:133] step: 996150, training_loss: 4.14641e+02\n",
            "I0408 16:48:16.981858 139805357463424 run_lib.py:133] step: 996200, training_loss: 5.89554e+02\n",
            "I0408 16:48:17.285734 139805357463424 run_lib.py:146] step: 996200, eval_loss: 1.15199e+03\n",
            "I0408 16:48:55.145195 139805357463424 run_lib.py:133] step: 996250, training_loss: 4.52808e+02\n",
            "I0408 16:49:32.966629 139805357463424 run_lib.py:133] step: 996300, training_loss: 5.01364e+02\n",
            "I0408 16:49:33.265866 139805357463424 run_lib.py:146] step: 996300, eval_loss: 1.13302e+03\n",
            "I0408 16:50:11.124221 139805357463424 run_lib.py:133] step: 996350, training_loss: 5.12044e+02\n",
            "I0408 16:50:48.997135 139805357463424 run_lib.py:133] step: 996400, training_loss: 1.54907e+03\n",
            "I0408 16:50:49.295185 139805357463424 run_lib.py:146] step: 996400, eval_loss: 3.29546e+02\n",
            "I0408 16:51:27.198009 139805357463424 run_lib.py:133] step: 996450, training_loss: 5.60511e+02\n",
            "I0408 16:52:05.030587 139805357463424 run_lib.py:133] step: 996500, training_loss: 3.09469e+02\n",
            "I0408 16:52:05.329170 139805357463424 run_lib.py:146] step: 996500, eval_loss: 3.57275e+02\n",
            "I0408 16:52:43.196438 139805357463424 run_lib.py:133] step: 996550, training_loss: 3.50515e+02\n",
            "I0408 16:53:21.048710 139805357463424 run_lib.py:133] step: 996600, training_loss: 1.02860e+03\n",
            "I0408 16:53:21.347259 139805357463424 run_lib.py:146] step: 996600, eval_loss: 1.12694e+03\n",
            "I0408 16:53:59.212928 139805357463424 run_lib.py:133] step: 996650, training_loss: 1.65897e+02\n",
            "I0408 16:54:37.060394 139805357463424 run_lib.py:133] step: 996700, training_loss: 4.64130e+02\n",
            "I0408 16:54:37.358364 139805357463424 run_lib.py:146] step: 996700, eval_loss: 6.30403e+02\n",
            "I0408 16:55:15.200636 139805357463424 run_lib.py:133] step: 996750, training_loss: 5.49223e+02\n",
            "I0408 16:55:53.068832 139805357463424 run_lib.py:133] step: 996800, training_loss: 3.82579e+02\n",
            "I0408 16:55:53.366818 139805357463424 run_lib.py:146] step: 996800, eval_loss: 6.05548e+02\n",
            "I0408 16:56:31.252668 139805357463424 run_lib.py:133] step: 996850, training_loss: 3.57082e+02\n",
            "I0408 16:57:09.114648 139805357463424 run_lib.py:133] step: 996900, training_loss: 8.63983e+02\n",
            "I0408 16:57:09.413869 139805357463424 run_lib.py:146] step: 996900, eval_loss: 8.79372e+02\n",
            "I0408 16:57:47.256978 139805357463424 run_lib.py:133] step: 996950, training_loss: 4.16877e+02\n",
            "I0408 16:58:25.102126 139805357463424 run_lib.py:133] step: 997000, training_loss: 5.68798e+02\n",
            "I0408 16:58:25.401587 139805357463424 run_lib.py:146] step: 997000, eval_loss: 2.14953e+02\n",
            "I0408 16:59:03.256804 139805357463424 run_lib.py:133] step: 997050, training_loss: 3.42113e+02\n",
            "I0408 16:59:41.102869 139805357463424 run_lib.py:133] step: 997100, training_loss: 8.72792e+02\n",
            "I0408 16:59:41.403294 139805357463424 run_lib.py:146] step: 997100, eval_loss: 2.88911e+02\n",
            "I0408 17:00:19.250483 139805357463424 run_lib.py:133] step: 997150, training_loss: 2.64766e+02\n",
            "I0408 17:00:57.105978 139805357463424 run_lib.py:133] step: 997200, training_loss: 4.14868e+02\n",
            "I0408 17:00:57.405123 139805357463424 run_lib.py:146] step: 997200, eval_loss: 4.82930e+02\n",
            "I0408 17:01:35.233854 139805357463424 run_lib.py:133] step: 997250, training_loss: 8.41104e+02\n",
            "I0408 17:02:13.095360 139805357463424 run_lib.py:133] step: 997300, training_loss: 5.78360e+02\n",
            "I0408 17:02:13.394687 139805357463424 run_lib.py:146] step: 997300, eval_loss: 7.59575e+02\n",
            "I0408 17:02:51.261334 139805357463424 run_lib.py:133] step: 997350, training_loss: 5.63395e+02\n",
            "I0408 17:03:29.119676 139805357463424 run_lib.py:133] step: 997400, training_loss: 3.70748e+02\n",
            "I0408 17:03:29.418789 139805357463424 run_lib.py:146] step: 997400, eval_loss: 3.82970e+02\n",
            "I0408 17:04:07.298977 139805357463424 run_lib.py:133] step: 997450, training_loss: 7.79194e+02\n",
            "I0408 17:04:45.146702 139805357463424 run_lib.py:133] step: 997500, training_loss: 7.91214e+02\n",
            "I0408 17:04:45.448295 139805357463424 run_lib.py:146] step: 997500, eval_loss: 6.03017e+02\n",
            "I0408 17:05:23.434776 139805357463424 run_lib.py:133] step: 997550, training_loss: 5.10298e+02\n",
            "I0408 17:06:01.300728 139805357463424 run_lib.py:133] step: 997600, training_loss: 1.36316e+03\n",
            "I0408 17:06:01.598555 139805357463424 run_lib.py:146] step: 997600, eval_loss: 8.18063e+02\n",
            "I0408 17:06:39.454525 139805357463424 run_lib.py:133] step: 997650, training_loss: 2.34153e+02\n",
            "I0408 17:07:17.343555 139805357463424 run_lib.py:133] step: 997700, training_loss: 4.21772e+02\n",
            "I0408 17:07:17.641918 139805357463424 run_lib.py:146] step: 997700, eval_loss: 7.78723e+02\n",
            "I0408 17:07:55.480446 139805357463424 run_lib.py:133] step: 997750, training_loss: 5.25753e+02\n",
            "I0408 17:08:33.331297 139805357463424 run_lib.py:133] step: 997800, training_loss: 3.34690e+02\n",
            "I0408 17:08:33.629214 139805357463424 run_lib.py:146] step: 997800, eval_loss: 6.80541e+02\n",
            "I0408 17:09:11.485683 139805357463424 run_lib.py:133] step: 997850, training_loss: 4.20130e+02\n",
            "I0408 17:09:49.351779 139805357463424 run_lib.py:133] step: 997900, training_loss: 1.67353e+03\n",
            "I0408 17:09:49.649703 139805357463424 run_lib.py:146] step: 997900, eval_loss: 4.73635e+02\n",
            "I0408 17:10:27.499505 139805357463424 run_lib.py:133] step: 997950, training_loss: 6.39244e+02\n",
            "I0408 17:11:05.344626 139805357463424 run_lib.py:133] step: 998000, training_loss: 9.89296e+02\n",
            "I0408 17:11:05.643109 139805357463424 run_lib.py:146] step: 998000, eval_loss: 4.33345e+02\n",
            "I0408 17:11:43.524139 139805357463424 run_lib.py:133] step: 998050, training_loss: 1.45435e+03\n",
            "I0408 17:12:21.392949 139805357463424 run_lib.py:133] step: 998100, training_loss: 8.96734e+02\n",
            "I0408 17:12:21.690987 139805357463424 run_lib.py:146] step: 998100, eval_loss: 4.06877e+02\n",
            "I0408 17:12:59.581484 139805357463424 run_lib.py:133] step: 998150, training_loss: 5.62396e+02\n",
            "I0408 17:13:37.449605 139805357463424 run_lib.py:133] step: 998200, training_loss: 1.94438e+02\n",
            "I0408 17:13:37.750605 139805357463424 run_lib.py:146] step: 998200, eval_loss: 3.53917e+02\n",
            "I0408 17:14:15.613739 139805357463424 run_lib.py:133] step: 998250, training_loss: 3.85648e+02\n",
            "I0408 17:14:53.548682 139805357463424 run_lib.py:133] step: 998300, training_loss: 3.39184e+02\n",
            "I0408 17:14:53.846695 139805357463424 run_lib.py:146] step: 998300, eval_loss: 4.69259e+02\n",
            "I0408 17:15:31.720819 139805357463424 run_lib.py:133] step: 998350, training_loss: 3.96825e+02\n",
            "I0408 17:16:09.609665 139805357463424 run_lib.py:133] step: 998400, training_loss: 6.15816e+02\n",
            "I0408 17:16:09.908286 139805357463424 run_lib.py:146] step: 998400, eval_loss: 3.08444e+02\n",
            "I0408 17:16:47.798457 139805357463424 run_lib.py:133] step: 998450, training_loss: 8.65202e+02\n",
            "I0408 17:17:25.645836 139805357463424 run_lib.py:133] step: 998500, training_loss: 7.04239e+02\n",
            "I0408 17:17:25.943407 139805357463424 run_lib.py:146] step: 998500, eval_loss: 5.35652e+02\n",
            "I0408 17:18:03.802850 139805357463424 run_lib.py:133] step: 998550, training_loss: 6.95701e+02\n",
            "I0408 17:18:41.666123 139805357463424 run_lib.py:133] step: 998600, training_loss: 3.59410e+02\n",
            "I0408 17:18:41.964624 139805357463424 run_lib.py:146] step: 998600, eval_loss: 9.65519e+01\n",
            "I0408 17:19:19.809400 139805357463424 run_lib.py:133] step: 998650, training_loss: 4.49915e+02\n",
            "I0408 17:19:57.655276 139805357463424 run_lib.py:133] step: 998700, training_loss: 7.73477e+02\n",
            "I0408 17:19:57.953431 139805357463424 run_lib.py:146] step: 998700, eval_loss: 4.99541e+02\n",
            "I0408 17:20:35.793841 139805357463424 run_lib.py:133] step: 998750, training_loss: 4.41361e+02\n",
            "I0408 17:21:13.698398 139805357463424 run_lib.py:133] step: 998800, training_loss: 3.68900e+02\n",
            "I0408 17:21:13.996531 139805357463424 run_lib.py:146] step: 998800, eval_loss: 5.99206e+02\n",
            "I0408 17:21:51.884587 139805357463424 run_lib.py:133] step: 998850, training_loss: 2.96474e+02\n",
            "I0408 17:22:29.788079 139805357463424 run_lib.py:133] step: 998900, training_loss: 4.67008e+02\n",
            "I0408 17:22:30.086691 139805357463424 run_lib.py:146] step: 998900, eval_loss: 6.48050e+02\n",
            "I0408 17:23:07.922475 139805357463424 run_lib.py:133] step: 998950, training_loss: 6.70329e+02\n",
            "I0408 17:23:45.782480 139805357463424 run_lib.py:133] step: 999000, training_loss: 7.25610e+02\n",
            "I0408 17:23:46.080997 139805357463424 run_lib.py:146] step: 999000, eval_loss: 3.04680e+02\n",
            "I0408 17:24:23.898017 139805357463424 run_lib.py:133] step: 999050, training_loss: 4.94763e+02\n",
            "I0408 17:25:01.744477 139805357463424 run_lib.py:133] step: 999100, training_loss: 1.68455e+02\n",
            "I0408 17:25:02.041994 139805357463424 run_lib.py:146] step: 999100, eval_loss: 1.04387e+03\n",
            "I0408 17:25:39.893959 139805357463424 run_lib.py:133] step: 999150, training_loss: 5.92963e+02\n",
            "I0408 17:26:17.744941 139805357463424 run_lib.py:133] step: 999200, training_loss: 4.69911e+02\n",
            "I0408 17:26:18.042943 139805357463424 run_lib.py:146] step: 999200, eval_loss: 9.59254e+02\n",
            "I0408 17:26:55.914019 139805357463424 run_lib.py:133] step: 999250, training_loss: 4.54558e+02\n",
            "I0408 17:27:33.796981 139805357463424 run_lib.py:133] step: 999300, training_loss: 3.41937e+02\n",
            "I0408 17:27:34.096044 139805357463424 run_lib.py:146] step: 999300, eval_loss: 8.60056e+02\n",
            "I0408 17:28:11.932040 139805357463424 run_lib.py:133] step: 999350, training_loss: 8.36403e+02\n",
            "I0408 17:28:49.776419 139805357463424 run_lib.py:133] step: 999400, training_loss: 2.56711e+02\n",
            "I0408 17:28:50.074305 139805357463424 run_lib.py:146] step: 999400, eval_loss: 4.99330e+02\n",
            "I0408 17:29:27.913869 139805357463424 run_lib.py:133] step: 999450, training_loss: 5.19664e+02\n",
            "I0408 17:30:05.738160 139805357463424 run_lib.py:133] step: 999500, training_loss: 2.52025e+02\n",
            "I0408 17:30:06.037191 139805357463424 run_lib.py:146] step: 999500, eval_loss: 8.77467e+02\n",
            "I0408 17:30:43.905102 139805357463424 run_lib.py:133] step: 999550, training_loss: 5.90661e+02\n",
            "I0408 17:31:21.752645 139805357463424 run_lib.py:133] step: 999600, training_loss: 3.47531e+02\n",
            "I0408 17:31:22.051889 139805357463424 run_lib.py:146] step: 999600, eval_loss: 1.35473e+03\n",
            "I0408 17:31:59.899570 139805357463424 run_lib.py:133] step: 999650, training_loss: 7.15951e+02\n",
            "I0408 17:32:37.767880 139805357463424 run_lib.py:133] step: 999700, training_loss: 3.24573e+02\n",
            "I0408 17:32:38.065359 139805357463424 run_lib.py:146] step: 999700, eval_loss: 9.87531e+02\n",
            "I0408 17:33:15.937713 139805357463424 run_lib.py:133] step: 999750, training_loss: 8.10444e+02\n",
            "I0408 17:33:53.759347 139805357463424 run_lib.py:133] step: 999800, training_loss: 6.70897e+02\n",
            "I0408 17:33:54.057269 139805357463424 run_lib.py:146] step: 999800, eval_loss: 4.35902e+02\n",
            "I0408 17:34:31.924985 139805357463424 run_lib.py:133] step: 999850, training_loss: 4.22501e+02\n",
            "I0408 17:35:09.738867 139805357463424 run_lib.py:133] step: 999900, training_loss: 6.01217e+02\n",
            "I0408 17:35:10.036456 139805357463424 run_lib.py:146] step: 999900, eval_loss: 3.44736e+02\n",
            "I0408 17:35:47.881036 139805357463424 run_lib.py:133] step: 999950, training_loss: 9.40924e+02\n",
            "I0408 17:36:25.728736 139805357463424 run_lib.py:133] step: 1000000, training_loss: 1.15495e+03\n",
            "I0408 17:36:30.312947 139805357463424 run_lib.py:146] step: 1000000, eval_loss: 8.54000e+02\n",
            "I0408 17:54:24.447958 139805357463424 run_lib.py:133] step: 1000050, training_loss: 7.93359e+02\n",
            "I0408 17:55:02.188925 139805357463424 run_lib.py:133] step: 1000100, training_loss: 4.27165e+02\n",
            "I0408 17:55:02.484839 139805357463424 run_lib.py:146] step: 1000100, eval_loss: 2.67529e+02\n",
            "I0408 17:55:40.196600 139805357463424 run_lib.py:133] step: 1000150, training_loss: 1.18012e+02\n",
            "I0408 17:56:17.909894 139805357463424 run_lib.py:133] step: 1000200, training_loss: 3.62157e+02\n",
            "I0408 17:56:18.207823 139805357463424 run_lib.py:146] step: 1000200, eval_loss: 2.41733e+02\n",
            "I0408 17:56:55.965138 139805357463424 run_lib.py:133] step: 1000250, training_loss: 9.03551e+02\n",
            "Traceback (most recent call last):\n",
            "  File \"main.py\", line 60, in <module>\n",
            "    app.run(main)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 300, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"main.py\", line 51, in main\n",
            "    run_lib.train(FLAGS.config, FLAGS.workdir)\n",
            "  File \"/content/thesis/score_sde_pytorch/run_lib.py\", line 131, in train\n",
            "    loss = train_step_fn(state, batch)\n",
            "  File \"/content/thesis/score_sde_pytorch/losses.py\", line 197, in step_fn\n",
            "    optimize_fn(optimizer, model.parameters(), step=state['step'])\n",
            "  File \"/content/thesis/score_sde_pytorch/losses.py\", line 49, in optimize_fn\n",
            "    torch.nn.utils.clip_grad_norm_(params, max_norm=grad_clip)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py\", line 36, in clip_grad_norm_\n",
            "    total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2L4aazIvBJ9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}